Automatically generated by Mendeley Desktop 1.16.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Thomas2012,
abstract = {Visual mixed and augmented realities have historically been applied to the gaming application domain. This article provides a survey of visual mixed and augmented reality gaming in both the academic and commercial contexts. There is an exploration of both indoor and outdoor mixed and augmented reality gaming. The different games are presented via the three major display technologies: head-mounted display, handheld display, and spatial immersive display. A number of academic mixed and augmented reality research projects are described that provide an overview of the current state of the art. A set of example commercial games are also examined to provide the context for the state of the games on the market.},
author = {Thomas, Bruce H.},
doi = {10.1145/2381876.2381879},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Games/MR{\_}AR{\_}Games.pdf:pdf},
isbn = {1544-3574},
issn = {15443574},
journal = {Computers in Entertainment},
keywords = {Augmented reality,and wearable computers,computer games,entertainment computing,mixed reality},
number = {3},
pages = {1--33},
title = {{A survey of visual, mixed, and augmented reality gaming}},
volume = {10},
year = {2012}
}
@phdthesis{Holloway1995,
abstract = {Augmented reality (AR) systems combine three-dimensional computer-generated imagery with the view of the real environment in order to make unseen objects visible or to present additional information. A critical problem is that the computer-generated objects do not currently remain correctly registered with the real environment–objects aligned from one viewpoint appear misaligned from another and appear to swim about as the viewer moves. This registration error is caused by a number of factors, such as system delay, optical distortion, and tracker measurement error, and is difficult to correct with existing technology. This dissertation presents a registration error model for AR systems and uses it to gain insight into the nature and severity of the registration error caused by the various error sources. My thesis is that a mathematical error model enables the system architect to determine which error sources are the most significant, the sensitivity of the net registration error to each error, the nature of the distortions caused by each type of error, the level of registration accuracy one can expect, and also provides insights on how best to calibrate the system. Analysis of a surgery planning application yielded the following main results: Even for moderate head velocities, system delay causes more registration error than all other sources combined; Using the eye's center of rotation as the eyepoint in the computer graphics model reduces the error due to eye rotation to zero for points along the line of gaze. This should obviate the need for eye tracking; Tracker error is a significant problem both in head tracking and in system calibration; The World coordinate system should be omitted when possible; Optical distortion is a significant error source, but correcting it computationally in the graphics pipeline often induces delay error larger than the distortion error itself; Knowledge of the nature of the various types of error facilitates identification and correction of errors in the calibration process. Although the model was developed for see-through head-mounted displays (STHMDs) for surgical planning, many of the results are applicable to other HMD systems as well.},
author = {Holloway, Richard Lee},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Image{\_}Recognition/Registration/10.1.1.50.377.pdf:pdf},
pages = {242},
school = {University of North Carolina},
title = {{Registration errors in augmented reality systems}},
year = {1995}
}
@article{Sindram2010,
abstract = {OBJECTIVES: Accurate laparoscopic liver lesion targeting for biopsy or ablation depends on the ability to merge laparoscopic and ultrasound images with proprioceptive instrument positioning, a skill that can be acquired only through extensive experience. The aim of this study was to determine whether using magnetic positional tracking to provide three-dimensional, real-time guidance improves accuracy during laparoscopic needle placement.$\backslash$n$\backslash$nMETHODS: Magnetic sensors were embedded into a needle and laparoscopic ultrasound transducer. These sensors interrupted the magnetic fields produced by an electromagnetic field generator, allowing for real-time, 3-D guidance on a stereoscopic monitor. Targets measuring 5 mm were embedded 3-5 cm deep in agar and placed inside a laparoscopic trainer box. Two novices (a college student and an intern) and two experts (hepatopancreatobiliary surgeons) targeted the lesions out of the ultrasound plane using either traditional or 3-D guidance.$\backslash$n$\backslash$nRESULTS: Each subject targeted 22 lesions, 11 with traditional and 11 with the novel guidance (n= 88). Hit rates of 32{\%} (14/44) and 100{\%} (44/44) were observed with the traditional approach and the 3-D magnetic guidance approach, respectively. The novices were essentially unable to hit the targets using the traditional approach, but did not miss using the novel system. The hit rate of experts improved from 59{\%} (13/22) to 100{\%} (22/22) (P {\textless} 0.0001).$\backslash$n$\backslash$nCONCLUSIONS: The novel magnetic 3-D laparoscopic ultrasound guidance results in perfect targeting of 5-mm lesions, even by surgical novices.},
author = {Sindram, David and McKillop, Iain H. and Martinie, John B. and Iannitti, David A.},
doi = {10.1111/j.1477-2574.2010.00244.x},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/Novel 3D.pdf:pdf},
isbn = {1477-2574 (Electronic) 1365-182X (Linking)},
issn = {1365182X},
journal = {Hpb},
keywords = {image guidance,laparoscopic,lesion targeting,ultrasound},
number = {10},
pages = {709--716},
pmid = {21083797},
title = {{Novel 3-D laparoscopic magnetic ultrasound image guidance for lesion targeting}},
volume = {12},
year = {2010}
}
@article{Mazuryk1996,
abstract = {Virtual Reality (VR), sometimes called Virtual Environments (VE) has drawn much attention in the last few years. Extensive media coverage causes this interest to grow rapidly. Very few people, however, really know what VR is, what its basic principles and its open problems are.  In this paper a historical overview of virtual reality is presented, basic terminology and classes of VR systems are listed, followed by applications of this technology in science, work, and entertainment areas. An insightful study of typical VR systems is done. All components of VR application and interrelations between them are thoroughly examined: input devices, output devices and software. Additionally human factors and their implication on the design issues of VE are discussed . Finally, the future of VR is considered in two aspects: technological and social. New research directions, technological frontiers and potential applications are pointed out. The possible positive and negative influence of VR on li...},
author = {Mazuryk, Tomasz and Gervautz, Michael},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Surveys/TR-186-2-96-06Paper.pdf:pdf},
pages = {72},
title = {{Virtual Reality - History, Applications, Technology and Future}},
url = {{"http://www.cg.tuwien.ac.at/research/publications/1996/mazuryk-1996-VRH/", {\}}$\backslash$nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.42.7849{\&}rep=rep1{\&}type=pdf$\backslash$nhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7849},
year = {1996}
}
@article{Rauschnabel2015,
abstract = {Recent market studies reveal that augmented reality (AR) devices, such as smart glasses, will substantially influence the media landscape. Yet, little is known about the intended adoption of smart glasses, particularly: Who are the early adopters of such wearables? We contribute to the growing body of research that investigates the role of personality in predicting media usage by analyzing smart glasses, such as Google Glass or Microsoft Hololens. First, we integrate AR devices into the current evolution of media and technologies. Then, we draw on the Big Five Model of human personality and present the results from two studies that investigate the direct and moderating effects of human personality on the awareness and innovation adoption of smart glasses. Our results show that open and emotionally stable consumers tend to be more aware of Google Glass. Consumers who perceive the potential for high functional benefits and social conformity of smart glasses are more likely to adopt such wearables. The strength of these effects is moderated by consumers' individual personality, particularly by their levels of openness to experience, extraversion and neuroticism. This article concludes with a discussion of theoretical and managerial implications for research on technology adoption, and with suggestions for avenues for future research.},
author = {Rauschnabel, Philipp A. and Brem, Alexander and Ivens, Bjoern S.},
doi = {10.1016/j.chb.2015.03.003},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Platform/HMD/1-s2.0-S0747563215001880-main.pdf:pdf},
isbn = {0747-5632},
issn = {07475632},
journal = {Computers in Human Behavior},
keywords = {Augmented reality,Google Glass,New media adoption,Personality,Smart glasses,Wearables},
pages = {635--647},
publisher = {Elsevier Ltd},
title = {{Who will buy smart glasses? Empirical results of two pre-market-entry studies on the role of personality in individual awareness and intended adoption of Google Glass wearables}},
url = {http://dx.doi.org/10.1016/j.chb.2015.03.003},
volume = {49},
year = {2015}
}
@article{PalocE;MaciaI;GomezR;BarandiaranIJimenezJm;etal.2004,
abstract = {Although augmented reality (AR) promises to provide valuable means for computer-aided surgery; the underlying technologies often create a cumbersome work environment that is inadequate for clinical employment. A great deal of research is still needed to develop comfortable and easy-to-use tools providing an augmented view of the patient and its main internal structures. We propose to develop an AR system for enhanced visualization of the liver that involves minimal annoyance for both the surgeon and the patient. The ultimate application of our system is to assist the surgeon in oncological liver surgery.},
author = {{Paloc E; Macia, I; Gomez, R; Barandiaran, I, Jimenez, Jm; et al.}, C; Carrasco},
doi = {10.1109/IV.2004.1320143},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/01320143.pdf:pdf},
isbn = {0-7695-2177-0},
issn = {1093-9547},
journal = {Infovis},
keywords = {--- augmented reality,assisted surgery,computer-,medical image segmentation},
pages = {189--193},
title = {{Computer-aided surgery based on auto-stereoscopic augmented reality}},
url = {http://ieeexplore.ieee.org/document/1320143/},
year = {2004}
}
@article{Debevec1998,
abstract = {We present a method that uses measured scene radiance and global illumination in order to add new objects to light-based models with correct lighting. The method uses a high dynamic range image- based model of the scene, rather than synthetic light sources, to il- luminate the new objects. To compute the illumination, the scene is considered as three components: the distant scene, the local scene, and the synthetic objects. The distant scene is assumed to be pho- tometrically unaffected by the objects, obviating the need for re- flectance model information. The local scene is endowed with es- timated reflectance model information so that it can catch shadows and receive reflected light from the new objects. Renderings are created with a standard global illumination method by simulating the interaction of light amongst the three components. A differen- tial rendering technique allows for good results to be obtained when only an estimate of the local scene reflectance properties is known. We apply the general method to the problem of rendering syn- thetic objects into real scenes. The light-based model is constructed from an approximate geometric model of the scene and by using a light probe to measure the incident illumination at the location of the synthetic objects. The global illumination solution is then com- posited into a photograph of the scene using the differential render- ing technique. We conclude by discussing the relevance of the tech- nique to recovering surface reflectance properties in uncontrolled lighting situations. Applications of the method include visual ef- fects, interior design, and architectural visualization. CR},
author = {Debevec, Paul},
doi = {10.1145/280814.280864},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Image{\_}Recognition/Illumination/e5a2.content.pdf:pdf},
isbn = {0897919998},
issn = {00978930},
journal = {In Computer Graphics Proceedings, Annual Conference Series (Proc. ACM SIGGRAPH '98 Proceeding)},
keywords = {ComputerGraphics,Digitization - Scanning,Image Processing,Scene Analysis - Photometry,Sensor Fusion,Three-Dimensional Graphics and Realism - Radiosity,Three-Dimensional Graphics and Realism-Color,Vision and Scene Understanding,and texture,color,photometry and thresholding,shading,shadowing},
pages = {189--198},
title = {{Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography}},
year = {1998}
}
@article{Lee2010,
abstract = {In the environment of mixed reality (MR) or augmented reality (AR), there have been several previous works dealing with user interfaces for manipulating and interacting with virtual objects aimed at improving immersive feeling and natural interaction. However, it is still considered that there must be much progress in supporting human behavior-like interactions for providing control efficiency and natural feeling in MR/AR environments. This paper proposes a tangible interaction method by combining the advantages of soft interactions such as hand gesture and MR and hard interactions such as vibro-tactile feedback. One of the main goals is to provide more natural interaction interfaces similar to the manipulation task in the real world by utilizing hand gesture-based tangible interactions. It also provides multimodal interfaces by adopting the vibro-tactile feedback and tangible interaction for the virtual object manipulation. Thus, it can make users get more immersive and natural feeling in the manipulation and interaction with virtual objects. Furthermore, it provides an alternative instruction guideline based on the analysis of the previous interaction while manipulating virtual objects, which makes it possible for the user to minimize manipulation errors during the interaction phase and the learning process, which guides the user to the right direction. We will show the effectiveness and advantage of the proposed approach by demonstrating several implementation results.},
author = {Lee, Jae Yeol and Rhee, Gue Won and Seo, Dong Woo},
doi = {10.1007/s00170-010-2671-x},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Input/art{\%}3A10.1007{\%}2Fs00170-010-2671-x.pdf:pdf},
issn = {02683768},
journal = {International Journal of Advanced Manufacturing Technology},
keywords = {Augmented reality,Hand gesture,Human-computer interface,Mixed reality,Pinch glove,Tangible interface,Vibro-tactile feedback},
number = {9-12},
pages = {1069--1082},
title = {{Hand gesture-based tangible interactions for manipulating virtual objects in a mixed reality environment}},
volume = {51},
year = {2010}
}
@article{Pandya2005,
author = {Pandya, Abhilash and Siadat, Mohammad-Reza and Auner, Greg},
doi = {10.3109/10929080500221626},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/Design implementation and accuracy of a prototype for medical augmented reality.pdf:pdf},
isbn = {1092-9088 (Print)$\backslash$r1092-9088 (Linking)},
issn = {1092-9088},
journal = {Computer Aided Surgery},
keywords = {*Robotics,*Surgery,*User-Computer Interface,Computer-Assisted,Humans,Image Processing,Imaging,Neuronavigation,Phantoms},
number = {1},
pages = {23--35},
pmid = {16199379},
title = {{Design, implementation and accuracy of a prototype for medical augmented reality}},
volume = {10},
year = {2005}
}
@incollection{Kiyokawa2016,
author = {Kiyoshi, Kiyokawa},
booktitle = {Fundamentals of Wearable Computers and Augmented Reality},
edition = {2nd},
editor = {Barfield, Woodrow},
pages = {59--84},
publisher = {CRC Press},
title = {{Head-Mounted display technologies for augmented reality}},
year = {2016}
}
@article{Soler2004,
abstract = {Medical image processing led to a major improvement of patient care: the 3D modeling of patients from their CT-scan or MRI provides an improved surgical planning and simulation allows to train the surgical gesture before carrying it out. These two preoperative steps can be used intra-operatively with the development of augmented reality (AR). In this paper, we present the tools we developed to provide our first prototypal AR guiding system for abdominal surgery.},
author = {Soler, L. and Nicolau, S. and Schmid, J. and Koehl, C. and Marescaux, J. and Pennec, X. and Ayache, N.},
doi = {10.1109/ISMAR.2004.64},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/01383075.pdf:pdf},
isbn = {0769521916},
journal = {ISMAR 2004: Proceedings of the Third IEEE and ACM International Symposium on Mixed and Augmented Reality},
number = {Ismar},
pages = {278--279},
title = {{Virtual reality and augmented reality in digestive surgery}},
url = {http://ieeexplore.ieee.org/document/1383075/},
year = {2004}
}
@article{Pence2010,
author = {Pence, Harry E.},
doi = {10.1080/02763877.2011.528281},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Games/Smartphones Smart Objects and Augmented Reality.pdf:pdf},
isbn = {02763877},
issn = {0276-3877},
journal = {The Reference Librarian},
number = {1-2},
pages = {136--145},
pmid = {57138001},
title = {{Smartphones, Smart Objects, and Augmented Reality}},
url = {http://www.tandfonline.com/doi/abs/10.1080/02763877.2011.528281},
volume = {52},
year = {2010}
}
@article{Starner2000,
abstract = {Computer gaming offers a unique test-bed and market for advanced concepts in computer science, such as Human Computer Interaction (HCI), computer-supported collaborative work (CSCW), intelligent agents, graphics, and sensing technology. In addition, computer gaming is especially well- suited for explorations in the relatively young fields of wearable computing and augmented reality (AR). This paper presents a developing multi-player augmented reality game, patterned as a cross between a martial arts fighting game and an agent controller, as implemented using the Wearable Augmented Reality for Personal, Intelligent, and Networked Gaming (WARPING) system. Through interactions based on gesture, voice, and head movement input and audio and graphical output, the WARPING system demonstrates how computer vision techniques can be exploited for advanced, intelligent interfaces.},
author = {Starner, Thad and Leibe, Bastian and Singletary, Brad and Pair, Jarrell},
doi = {10.1145/325737.325864},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Games/10.1.1.226.753.pdf:pdf},
isbn = {1581131348},
journal = {Proceedings of the 5th international conference on Intelligent user interfaces},
keywords = {augmented reality,computer vision,wearable computing},
pages = {256--259},
title = {{Mind-warping: towards creating a compelling collaborative augmented reality game}},
year = {2000}
}
@article{Bajura1995,
abstract = {This paper addresses the problem of correcting visual regis- tration errors in video-based augmented-reality systems. Accurate visual registration between real and computer- generated objects in combined images is critically important for conveying the perception that both types of object occupy the same 3-dimensional (3D) space. To date, augmented- reality systems have concentrated on simply improving 3D coordinate system registration in order to improve apparent (image) registration error. This paper introduces the the idea of dynamically measuring registration error in combined im- ages (2D error) and using that information to correct 3D coordinate system registration error which in turn improves registration in the combined images. Registration can be made exact in every combined image if a small video delay can be tolerated. Our experimental augmented-reality sys- tem achieves improved image registration, stability, and error tolerance from tracking system drift andjitter over cur- rent augmented-reality systems. No additional tracking hardware or other devices are needed on the user's head- mounted display. Computer-generated objects can be “nailed” to real-world reference points in every image the user sees with an easily-implemented algorithm. Dynamic error correction as demonstrated here will likely be a key component of future augmented-reality systems.},
author = {Bajura, Michael and Neumann, Ulrich},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Image{\_}Recognition/Registration/DynamicRegistrationVRAIS95.pdf:pdf},
isbn = {0818670843},
journal = {IEEE Computer Graphics and Applications},
keywords = {Augmented Reality,Registration.,Virtual Reality},
number = {5},
pages = {52--60},
title = {{Dynamic registration correction in video-based augmented reality systems}},
volume = {15},
year = {1995}
}
@article{Billinghurst2015,
abstract = {This survey summarizes almost 50 years of research and development in the field of Augmented Reality (AR). From early research in the1960's until widespread availability by the 2010's there has been steady progress towards the goal of being able to seamlessly combine real and virtual worlds. We provide an overview of the common definitions of AR, and show how AR fits into taxonomies of other related technologies. A history of important milestones in Augmented Reality is followed by sections on the key enabling technologies of tracking, display and input devices. We also review design guidelines and provide some examples of successful AR applications. Finally, we conclude with a summary of directions for future work and a review of some of the areas that are currently being researched.},
author = {Billinghurst, Mark and Clark, Adrian and Lee, Gun},
doi = {10.1561/1100000049},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Surveys/A{\_}Survey{\_}of{\_}Augmented{\_}Reality.pdf:pdf},
isbn = {1551-3955},
issn = {1551-3955},
journal = {Foundations and Trends in Human-Computer Interaction},
number = {2-3},
pages = {73--272},
title = {{A survey of augmented reality}},
volume = {8},
year = {2015}
}
@article{Berryman2012,
author = {Berryman, Donna R.},
doi = {10.1080/02763869.2012.670604},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Surveys/Augmented Reality A Review.pdf:pdf},
isbn = {0276-3869},
issn = {0276-3869},
journal = {Medical Reference Services Quarterly},
number = {2},
pages = {212--218},
pmid = {22559183},
title = {{Augmented Reality: A Review}},
volume = {31},
year = {2012}
}
@book{Bimber2005,
abstract = {Like virtual reality, augmented reality is becoming an emerging platform in new application areas for museums, edutainment, home entertainment, research, industry, and the art communities using novel approaches which have taken augmented reality beyond traditional eye-worn or hand-held displays. In this book, the authors discuss spatial augmented reality approaches that exploit optical elements, video projectors, holograms, radio frequency tags, and tracking technology, as well as interactive rendering algorithms and calibration techniques in order to embed synthetic supplements into the real environment or into a live video of the real environment. Special Features Comprehensive overview Detailed mathematical equations Code fragments Implementation instructions Examples of Spatial AR displays},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bimber, Oliver and Raskar, Ramesh},
booktitle = {CRC press},
doi = {10.1260/147807708784640126},
eprint = {arXiv:1011.1669v3},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Platform/SAR/SAR.pdf:pdf},
isbn = {1568812302},
issn = {14780771},
pages = {341},
pmid = {13859354},
title = {{Spatial augmented reality merging real and virtual worlds}},
year = {2005}
}
@article{Abe2013,
abstract = {Augmented reality (AR) is an imaging technology by which virtual objects are overlaid onto images of real objects captured in real time by a tracking camera. This study aimed to introduce a novel AR guidance system called virtual protractor with augmented reality (VIPAR) to visualize a needle trajectory in 3D space during percutaneous vertebroplasty (PVP). The AR system used for this study comprised a head-mount display (HMD) with a tracking camera and a marker sheet. An augmented scene was created by overlaying the preoperatively generated needle trajectory path onto a marker detected on the patient using AR software, thereby providing the surgeon with augmented views in real time through the HMD. The accuracy of the system was evaluated by using a computer-generated simulation model in a spine phantom and also evaluated clinically in 5 patients. In the 40 spine phantom trials, the error of the insertion angle (EIA), defined as the difference between the attempted angle and the insertion angle, was evaluated using 3D CT scanning. Computed tomography analysis of the 40 spine phantom trials showed that the EIA in the axial plane significantly improved when VIPAR was used compared with when it was not used (0.96° ± 0.61° vs 4.34° ± 2.36°, respectively). The same held true for EIA in the sagittal plane (0.61° ± 0.70° vs 2.55° ± 1.93°, respectively). In the clinical evaluation of the AR system, 5 patients with osteoporotic vertebral fractures underwent VIPARguided PVP from October 2011 to May 2012. The postoperative EIA was evaluated using CT. The clinical results of the 5 patients showed that the EIA in all 10 needle insertions was 2.09° ± 1.3° in the axial plane and 1.98° ± 1.8° in the sagittal plane. There was no pedicle breach or leakage of polymethylmethacrylate. VIPAR was successfully used to assist in needle insertion during PVP by providing the surgeon with an ideal insertion point and needle trajectory through the HMD. The findings indicate that AR guidance technology can become a useful assistive device during spine surgeries requiring percutaneous procedures.},
author = {Abe, Yuichiro and Sato, Shigenobu and Kato, Koji and Hyakumachi, Takahiko and Yanagibashi, Yasushi and Ito, Manabu and Abumi, Kuniyoshi},
doi = {10.3171/2013.7.SPINE12917},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/SpineAR.pdf:pdf},
journal = {Journal of Neurosurgery: Spine},
keywords = {Augmented reality,Computer-assisted surgery,Percutaneous spinal approach,Percutaneous vertebroplasty,Technique},
number = {4},
pages = {492--501},
title = {{A novel 3D guidance system using augmented reality for percutaneous vertebroplasty}},
volume = {19},
year = {2013}
}
@article{Hollerer2004,
author = {Hollerer, Tobias},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Input/hollerer-2004-diss.pdf:pdf},
journal = {Graduate School of Arts and Sciences},
keywords = {AR: Design for AR,AR: Mobile AR,AR: Outdoor,Augmented Interaction,PD Library,Thesis - PhD},
pages = {238},
title = {{PD557 User interfaces for mobile augmented reality}},
volume = {PhD},
year = {2004}
}
@article{Foley2001,
author = {Foley, Kevin T and Simon, David A and Rampersaud, Y Raja},
doi = {10.1097/00007632-200102150-00009},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/00007632-200102150-00009.pdf:pdf},
issn = {0362-2436},
journal = {Spine},
keywords = {Computer-Assisted,Fluoroscopy,Fluoroscopy: instrumentation,Fluoroscopy: methods,Humans,Image Processing,Spinal Diseases,Spinal Diseases: surgery,Spine,Spine: surgery,User-Computer Interface},
number = {4},
pages = {347--351},
pmid = {11224880},
title = {{Virtual fluoroscopy: computer-assisted fluoroscopic navigation}},
volume = {26},
year = {2001}
}
@article{Joskowicz2006,
abstract = {This paper describes a novel image-guided system for precise automatic targeting in minimally invasive keyhole neurosurgery. The system consists of the MARS miniature robot fitted with a mechanical guide for needle, probe or catheter insertion. Intraoperatively, the robot is directly affixed to a head clamp or to the patient's skull. It automatically positions itself with respect to predefined targets in a preoperative CT/MRI image following an anatomical registration with an intraoperative 3D surface scan of the patient's facial features and registration jig. We present the system architecture, surgical protocol, custom hardware (targeting and registration jig), and software modules (preoperative planning, intraoperative execution, 3D surface scan processing, and three-way registration). We also describe a prototype implementation of the system and in vitro registration experiments. Our results indicate a system-wide target registration error of 1.7 mm (standard deviation = 0.7 mm), which is close to the required 1.0-1.5 mm clinical accuracy in many keyhole neurosurgical procedures.},
author = {Joskowicz, L and Shamir, R and Freiman, M and Shoham, M and Zehavi, E and Umansky, F and Shoshan, Y},
doi = {10.3109/10929080600909351},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/Image guided system with miniature robot for precise positioning and targeting in keyhole neurosurgery.pdf:pdf},
isbn = {1092-9088 (Print)$\backslash$r1092-9088 (Linking)},
issn = {1092-9088},
journal = {Computer aided surgery : official journal of the International Society for Computer Aided Surgery},
keywords = {computer-aided neurosurgery,medical robotics,multimodal registration},
number = {4},
pages = {181--193},
pmid = {17038306},
title = {{Image-guided system with miniature robot for precise positioning and targeting in keyhole neurosurgery.}},
volume = {11},
year = {2006}
}
@article{Lacey2007,
abstract = {Cited in PD393},
author = {Lacey, Gerard and Ryan, Donncha and Cassidy, Derek and Young, Derek},
doi = {10.1109/MMUL.2007.79},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/04354160.pdf:pdf},
issn = {1070986X},
journal = {IEEE Multimedia},
keywords = {Augmented reality,Surgical training,Virtual reality and psychomotor skills assessment},
number = {4},
pages = {76--87},
title = {{Mixed-reality simulation of minimally invasive surgeries}},
url = {http://ieeexplore.ieee.org/document/4354160/},
volume = {14},
year = {2007}
}
@article{Kojima2006,
abstract = {We propose a novel display based game environment using augmented reality technology with small robots. In this environment, the small robots can be augmented by a display image according to their positions and postures. The augmentation activity reinforces the fun of playing with such small robots in the real world.},
author = {Kojima, Minoru and Sugimoto, Maki and Nakamura, Akihiro and Tomita, Masahiro and Nii, Hideaki and Inami, Masahiko},
doi = {10.1109/TABLETOP.2006.3},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Games/01579184.pdf:pdf},
isbn = {076952494X},
journal = {Proceedings of the First IEEE International Workshop on Horizontal Interactive Human-Computer Systems, TABLETOP'06},
pages = {3--8},
title = {{Augmented coliseum: An augmented game environment with small vehicles}},
volume = {2006},
year = {2006}
}
@article{Azad,
author = {Azad, Sasha and Saldanha, Carl and Gan, Cheng Hann and Riedl, Mark O},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Games/aiide-exag16.pdf:pdf},
journal = {Twelfth Artificial Intelligence and Interactive Digital Entertainment Conference},
title = {{Mixed Reality Meets Procedural Content Generation in Video Games}},
year = {2016}
}
@book{Barfield2016,
abstract = {Fundamentals of Wearable Computers and Augmented Reality presents a broad coverage of the technologies and interface design issues associated with wearable computers and augmented reality displays both rapidly developing fields in computer science, engineering, and human interface design. This book presents concepts related to the use and underlying technologies of augmented reality and wearable computer systems. There are many application areas for this technology, such as medicine, manufacturing, training, and recreation. Wearable computers will allow a much closer association of information with the user than is possible with traditional desktop computers. This book addresses an important aspect of wearable computers and augmented reality, either from the conceptual or from an application framework. Given the wide coverage of topics on issues related to the display of computer-generated images in the environment, this book can be used as a text for computer science, computer engineering, and interface design courses.},
author = {Barfield, Woodrow},
booktitle = {CRC Press},
doi = {10.1162/PRES_r_00244},
edition = {2},
editor = {B, Woodrow},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Surveys/Fundamentals of Wearable Computers and Augmented Reality (2nd Ed)(gnv64).pdf:pdf},
isbn = {978-1-4822-4351-2},
issn = {1054-7460},
publisher = {CRC Press},
title = {{Fundamentals of Wearable Computers and Augmented Reality, Second Edition}},
year = {2015}
}
@article{Zhou2008,
author = {Zhou, Feng and Duh, Henry Been-lirn and Billinghurst, Mark},
doi = {10.1109/ISMAR.2008.4637362},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Input/2345.pdf:pdf},
isbn = {978-1-4244-2840-3},
journal = {IEEE Computer Society},
pages = {193--202},
title = {{Trends in augmented reality tracking, interaction and display: A review of ten years of ISMAR}},
year = {2008}
}
@article{Cavazza2003,
author = {Cavazza, Marc and Martin, Olivier and Charles, Fred and Marichal, Xavier and Mead, Steven J},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Games/100381.pdf:pdf},
journal = {Proceedings of the 2nd IEEE and ACM International Symposium on Mixed and Augmented Reality},
pages = {304},
title = {{User interaction in mixed reality interactive storytelling}},
year = {2003}
}
@article{Azad2016,
author = {Azad, Sasha and Saldanha, Carl and Gan, Cheng-hann and Riedl, Mark O},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Games/azad-aiide16.pdf:pdf},
title = {{Procedural Level Generation for Augmented Reality Games}},
year = {2015}
}
@misc{Hager2006,
author = {Hager, Gregory D and Wegbreit, Eliot Leonard},
doi = {10.1126/science.Liquids},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/US7103212.pdf:pdf},
institution = {Strider Labs, Inc},
isbn = {3723429343},
number = {12},
pages = {13},
publisher = {U.S. Patent},
title = {{Acquisition of three-dimensional images by an active stereo technique using locally unique patterns}},
volume = {2},
year = {2006}
}
@article{Navab2012,
author = {Navab, Nassir and Blum, Tobias and Wang, Lejing and Okur, Asli and Wendler, Thomas},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/06165241.pdf:pdf},
isbn = {0018-9162},
journal = {Computer},
number = {2},
pages = {48--55},
title = {{First deployments of augmented reality in operating rooms}},
volume = {45},
year = {2012}
}
@article{Thomas2000,
abstract = {This paper presents a first person outdoor/indoor augmented reality application ARQuake that we have developed. ARQuake is an extension of the desktop game Quake, and as such we are investigating how to convert a desktop first person application into an outdoor/indoor mobile augmented reality application. We present an architecture for a low cost, moderately accurate six degrees of freedom tracking system based on GPS, digital compass, and fiducial vision-based tracking. Usability issues such as monster selection, colour, input devices, and multi-person collaboration are discussed.},
author = {Thomas, Bruce and Close, Ben and Donoghue, John and Squires, John and {De Bondi}, Phillip and Piekarski, Wayne and Morris, Michael},
doi = {10.1007/s007790200007},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Games/thomas-iswc-2000.pdf:pdf},
isbn = {0769507956},
issn = {16174909},
journal = {Personal and Ubiquitous Computing},
keywords = {Augmented reality,Computer games,Wearable computers},
number = {1},
pages = {75--86},
title = {{ARQuake: An outdoor/indoor augmented reality first person application}},
volume = {6},
year = {2000}
}
@article{Hosoi2007,
abstract = {An example of an application enhanced by the "manipulation-by-projection" technique, this cooperative game allows players to visually and intuitively control a robot with projectors. Players interchangeably move and connect their projected images to create a path that leads the robot to its goal.},
author = {Hosoi, Kazuhiro and Dao, V.N. and Mori, A. and Sugimoto, M.},
doi = {10.1145/1278280.1278283},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Games/10.1.1.469.6769.pdf:pdf},
isbn = {9781450318242},
journal = {ACM SIGGRAPH 2007 emerging technologies},
title = {{CoGAME: manipulation using a handheld projector}},
year = {2007}
}
@article{Piekarski2002,
author = {Piekarski, Wayne and Thomas, Bruce},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Games/10.1.1.3.5679.pdf:pdf},
journal = {Communications of the ACM},
number = {1},
pages = {36--38},
title = {{Arquake: the outdoor augmented reality gaming system}},
volume = {45},
year = {2002}
}
@article{Carmigniani2011,
abstract = {This paper surveys the current state-of-the-art of technology, systems and applications in Augmented Reality. It describes work performed by many different research groups, the purpose behind each new Augmented Reality system, and the difficulties and problems encountered when building some Augmented Reality applications. It surveysmobile augmented reality systems challenges and requirements for successful mobile systems. This paper summarizes the current applications of Augmented Reality and speculates on future applications and where current research will lead Augmented Reality's development. Challenges augmented reality is facing in each of these applications to go fromthe laboratories to the industry, as well as the future challenges we can forecast are also discussed in this paper. Section 1 gives an introduction to what Augmented Reality is and the motivations for developing this technology. Section 2 discusses Augmented Reality Technologies with computer vision methods, AR devices, interfaces and systems, and visualization tools. The mobile and wireless systems for Augmented Reality are discussed in Section 3. Four classes of current applications that have been explored are described in Section 4. These applications were chosen as they are the most famous type of applications encountered when researching AR apps. The future of augmented reality and the challenges they will be facing are discussed in Section 5.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Carmigniani, Julie and Furht, Borko and Anisetti, Marco and Ceravolo, Paolo and Damiani, Ernesto and Ivkovic, Misa},
doi = {10.1007/s11042-010-0660-6},
eprint = {1512.00567},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Surveys/art{\%}3A10.1007{\%}2Fs11042-010-0660-6.pdf:pdf},
isbn = {13807501},
issn = {13807501},
journal = {Multimedia Tools and Applications},
keywords = {AR,Augmented reality,Augmented reality applications,Augmented reality iphone4,Augmented reality on mobile devices,Augmented reality systems,Augmented reality technologies},
number = {1},
pages = {341--377},
pmid = {8190083},
title = {{Augmented reality technologies, systems and applications}},
volume = {51},
year = {2011}
}
@article{Krevelen2010,
abstract = {We are on the verge of ubiquitously adopting Augmented Reality (AR) technologies to enhance our perception and help us see, hear, and feel our environments in new and enriched ways. AR will support us in fields such as education, maintenance, design and reconnaissance, to name but a few. This paper describes the field of AR, including a brief definition and development history, the enabling technologies and their characteristics. It surveys the state of the art by reviewing some recent applications of AR technology as well as some known limitations regarding human factors in the use of AR systems that developers will need to overcome.},
author = {Krevelen, D.W.F. Van and Poelman, R.},
doi = {10.1155/2011/721827},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Surveys/paper1 .pdf:pdf},
isbn = {1081-1451},
issn = {0353-4707},
journal = {International Journal of Virtual Reality},
keywords = {augmented reality,challenges,tracking},
number = {2},
pages = {29--46},
title = {{A survey of augmented reality technologies, applications and limitations}},
volume = {9},
year = {2010}
}
@incollection{Mountneyr2009,
abstract = {The registration of various data is a challenging task in medical image processing and a highly frequented area of research. Most of the published approaches tend to fail sporadically on different data sets. This happens due to two major problems. First, local optimization strategies induce a high risk when optimizing nonconvex functions. Second, similarity measures might fail if they are not suitable for the data. Thus, researchers began to combine multiple measures by weighted sums. In this paper, we show severe limitations of such summation approaches. We address both issues by a gradient-based vector optimization algorithm that uses multiple similarity measures. It gathers context information from the iteration process to detect and suppress failing measures. The new approach is evaluated by experiments from the field of 2D-3D registration. Besides its generic character with respect to arbitrary data, the main benefit is a highly robust iteration behavior, where even very poor initial guesses of the transform result in good solutions. {\textcopyright} 2009 Springer-Verlag.},
author = {{Mountney, Peter and Giannarou, Stamatia and Elson, Daniel and Yang}, Guang-Zhong},
booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
doi = {10.1007/978-3-642-04268-3},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/Optical{\_}biopsy{\_}mapping{\_}for{\_}minimally{\_}invasive{\_}cancer{\_}screening.pdf:pdf},
isbn = {978-3-642-04267-6},
issn = {03029743},
pages = {483--490},
pmid = {20426110},
title = {{Optical Biopsy Mapping for Minimally Invasive Cancer Screening Peter}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-77952254907{\&}partnerID=tZOtx3y1},
year = {2009}
}
@article{Jones2013,
abstract = {IllumiRoom is a proof-of-concept system that augments the area surrounding a television with projected visualizations to enhance traditional gaming experiences. We investigate how projected visualizations in the periphery can negate, include, or augment the existing physical environment and complement the content displayed on the television screen. Peripheral projected illusions can change the appearance of the room, induce apparent motion, extend the field of view, and enable entirely new physical gaming experiences. Our system is entirely self-calibrating and is designed to work in any room. We present a detailed exploration of the design space of peripheral projected illusions and we demonstrate ways to trigger and drive such illusions from gaming content. We also contribute specific feedback from two groups of target users (10 gamers and 15 game designers); providing insights for enhancing game experiences through peripheral projected illusions.},
author = {Jones, Brett R. and Benko, Hrvoje and Ofek, Eyal and Wilson, Andrew D.},
doi = {10.1145/2470654.2466112},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Platform/SAR/illumiroom-illumiroom{\_}chi2013{\_}bjones.pdf:pdf},
isbn = {9781450318990},
issn = {9781450318990},
journal = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
pages = {869},
title = {{IllumiRoom: Peripheral Projected Illusions for Interactive Experiences}},
year = {2013}
}
@incollection{Yaniv2015,
author = {Yaniv, Ziv and Linte, Cristian A.},
booktitle = {Fundamentals of Wearable Computers and Augmented Reality},
chapter = {19},
edition = {2},
editor = {Barfield, Woodrow},
pages = {485 -- 518},
publisher = {CRC Press},
title = {{Applications of augmented reality in the operating room}},
year = {2015}
}
@article{Hoshi1996,
author = {Hoshi, Hiroaki and Taniguchi, Naosato and Morishima, Hideki and Akiyama, Takeshi and Yamazaki, Shouichi and Okuyama, Atsushi},
journal = {Electronic Imaging: Science {\&} Technology},
pages = {234--242},
title = {{Off-axial HMD optical system consisting of aspherical surfaces without rotational symmetry}},
year = {1996}
}
@article{Navab2010,
abstract = {Mobile C-arm is an essential tool in everyday trauma and orthopedics surgery. Minimally invasive solutions, based on X-ray imaging and coregistered external navigation created a lot of interest within the surgical community and started to replace the traditional open surgery for many procedures. These solutions usually increase the accuracy and reduce the trauma. In general, they introduce new hardware into the OR and add the line of sight constraints imposed by optical tracking systems. They thus impose radical changes to the surgical setup and overall procedure. We augment a commonly used mobile C-arm with a standard video camera and a double mirror system allowing real-time fusion of optical and X-ray images. The video camera is mounted such that its optical center virtually coincides with the C-arm's X-ray source. After a one-time calibration routine, the acquired X-ray and optical images are coregistered. This paper describes the design of such a system, quantifies its technical accuracy, and provides a qualitative proof of its efficiency through cadaver studies conducted by trauma surgeons. In particular, it studies the relevance of this system for surgical navigation within pedicle screw placement, vertebroplasty, and intramedullary nail locking procedures. The image overlay provides an intuitive interface for surgical guidance with an accuracy of {\textless} 1 mm, ideally with the use of only one single X-ray image. The new system is smoothly integrated into the clinical application with no additional hardware especially for down-the-beam instrument guidance based on the anteroposterior oblique view, where the instrument axis is aligned with the X-ray source. Throughout all experiments, the camera augmented mobile C-arm system proved to be an intuitive and robust guidance solution for selected clinical routines.},
author = {Navab, Nassir and Heining, Sandro Michael and Traub, Joerg},
doi = {10.1109/TMI.2009.2021947},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/10.1.1.174.9260.pdf:pdf},
isbn = {1558-254X (Electronic)$\backslash$r0278-0062 (Linking)},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Augmented reality visualization,C-arm navigation,image-guided surgery},
number = {7},
pages = {1412--1423},
pmid = {20659830},
title = {{Camera augmented mobile C-arm (CAMC): Calibration, accuracy study, and clinical applications}},
volume = {29},
year = {2010}
}
@article{Jia2010,
abstract = {Fiducial marker systems consist of unique patterns mounted in the environment and computer vision algorithms that help automatically find features in digital camera images. They are useful for Augmented Reality (AR), robot navigation, 3D modeling, and other applications. In this paper we introduce an extension of marker-based approaches. For some applications which need a large working space, we mounted markers onto the ceiling. Two cameras are fixed onto the head-mounted displays (one captures the markers and the other captures the real scene). After calibration of the two cameras, the pose of the camera captures the real scene can be obtained to merge virtual scene.},
author = {Jia, Jun and Qi, Yue and Zuo, Qing},
doi = {10.1109/WMSVM.2010.52},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Head{\_}Tracking/05558353.pdf:pdf},
isbn = {9780769540467},
journal = {2nd International Conference on Modeling, Simulation, and Visualization Methods, WMSVM 2010},
keywords = {Artoolkitplus,Augmented reality,Calibration,Marker-based,Tracking},
pages = {94--97},
title = {{An extended marker-based tracking system for augmented reality}},
year = {2010}
}
@incollection{Mukaigawa1999,
abstract = {A concept named Photometric Image-Based Rendering (PIBR) is introduced for a seamless augmented reality. The PIBR is defined as image-based rendering which covers appearance changes caused by the lighting condition changes, while Geometric Image-Based Rendering (GIBR) is defined as image-based rendering which covers appearance changes caused by the view point changes. PIBR can be applied to image synthesis to keep photometric consistency between virtual objects and real scenes in arbitrary lighting conditions. We analyze conventional IBR algorithms, and formalize PIBR within the whole IBR framework. A specific algorithm is also presented for realizing PIBR. The photometric linearization makes a controllable framework for PIBR, which consists of four processes: (1) separation of environmental illumination effects, (2) estimation of lighting directions, (3) separation of specular reflections and cast-shadows, and (4) linearization of self-shadows. After the-photometric linearization of input images, we can synthesize any realistic images which include not-only diffuse reflections but also self-shadows, cast-shadows and specular reflections. Experimental results show that realistic images can be successfully synthesized while keeping photometric consistency},
author = {Mukaigawa, Y and Mihashi, S and Shakunaga, T},
booktitle = {Augmented Reality, 1999. (IWAR '99) Proceedings. 2nd IEEE and ACM International Workshop on},
doi = {10.1109/IWAR.1999.803812},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Image{\_}Recognition/Illumination/IWAR99.pdf:pdf},
isbn = {0-7695-0359-4},
keywords = {augmented reality;lighting;realistic images;render},
pages = {115--124},
title = {{Photometric image-based rendering for virtual lighting image synthesis}},
url = {http://ieeexplore.ieee.org/document/803812/},
year = {1999}
}
@misc{Fisher1996,
abstract = {A head-mounted projection display system featuring a beam splitter displays a simulated environment to an observer using a light-weight, low-cost, head-mounted projector and a retro-reflective screen. The display system optically co-locates the projector with the observer's eyes for effective use of either curved or flat retro-reflective screens. High screen gain achieved by the head-mounted projection display system makes inexpensive projector sources such as a cathode ray tube feasible. An alternative head-mounted display system also incorporating beam splitters produces an unlimited horizontal field of view, but with limited binocular overlap, while using multiple head-mounted image sources for each eye. A method of providing the head-mounted display system also is disclosed},
author = {Fisher, Ralph W.},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Platform/HMD/US5572229.pdf:pdf},
publisher = {Google Patents},
title = {{Head-mounted projection display system featuring beam splitter and method of making same}},
year = {1996}
}
@article{Chaves2012,
abstract = {The computational implementation of human body gestures recognition has been a challenge for several years. Nowadays, thanks to the development of RGB-D cameras it is possible to acquire a set of data that represents a human position in time. Despite that, these cameras provide raw data, still being a problem to identify in real-time a specific pre-defined user movement without relying on offline training. However, in several cases the real-time requisite is critical, especially when it is necessary to detect and analyze a movement continuously, as in the tracking of physiotherapeutic movements or exercises. This paper presents a simple and fast technique to recognize human movements using the set of data provided by a RGB-D camera. Moreover, it describes a way to identify not only if the performed motion is valid, i.e. belongs to a set of pre-defined gestures, but also the identification of at which point the motion is (beginning, end or somewhere in the middle of it). The precision of the proposed technique can be set to suit the needs of the application and has a simple and fast way of gesture registration, thus, being easy to set new motions if necessary. The proposed technique has been validated through a set of tests focused on analyzing its robustness considering a series of variations during the interaction like fast and complex gestures.},
author = {Chaves, Thiago and Figueiredo, Lucas and Gama, Alana Da and Ara{\'{u}}jo, Cristiano De and Teichrieb, Veronica},
doi = {10.1109/SVR.2012.16},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Input/06297539.pdf:pdf},
isbn = {9780769547251},
journal = {Proceedings - 2012 14th Symposium on Virtual and Augmented Reality, SVR 2012},
keywords = {Body Tracking,Human motion analysis,Kinect,RGB-D camera},
pages = {271--278},
title = {{Human body motion and gestures recognition based on checkpoints}},
year = {2012}
}
@phdthesis{Klein2006,
abstract = {In Augmented Reality applications, the real environment is annotated or enhanced with computer-generated graphics. These graphics must be exactly registered to real objects in the scene and this requires AR systems to track a users viewpoint. This thesis shows that visual tracking with inexpensive cameras (such as those now often built into mobile computing devices) can be sufficiently robust and accurate for AR applications. Visual tracking has previously been applied to AR, however this has used artificial markers placed in the scene; this is undesirable and this thesis shows that it is no longer necessary. To address the demanding tracking needs of AR, two specific AR formats are considered. Firstly, for a head-mounted display, a markerless tracker which is robust to rapid head motions is presented. This robustness is achieved by combining visual measurements with those of head-worn inertial sensors. A novel sensor fusion approach allows not only pose prediction, but also enables the tracking of video with unprecedented levels of motion blur. Secondly, the tablet PC is proposed as a user-friendly ARmedium. For this device, tracking combines inside-out edge tracking with outside-in tracking of tablet-mounted LEDs. Through the external fusion of these complementary sensors, accurate and robust tracking is achieved within a modest computing budget. This allows further visual analysis of the occlusion boundaries between real and virtual objects and a marked improvement in the quality of augmentations. Finally, this thesis shows that not only can tracking be made resilient to motion blur, it can benefit from it. By exploiting the directional nature of motion blur, camera rotations can be extracted from individual blurred frames. The extreme efficiency of the proposed method makes it a viable drop-in replacement for inertial sensors.},
author = {Klein, Georg},
doi = {10.1109/IPIN.2010.5648274},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Image{\_}Recognition/Visual{\_}Tracking{\_}for{\_}Augmented{\_}Reality.pdf:pdf},
isbn = {9781424458646},
issn = {15324192},
number = {January},
pages = {193},
pmid = {19909018},
school = {University of Cambridge},
title = {{Visual tracking for augmented reality}},
year = {2006}
}
@article{Kockro2009,
abstract = {OBJECTIVE: We developed an augmented reality system that enables intraoperative image guidance by using 3-dimensional (3D) graphics overlaid on a video stream. We call this system DEX-Ray and report on its development and the initial intraoperative experience in 12 cases. METHODS: DEX-Ray consists of a tracked handheld probe that integrates a lipstick-size video camera. The camera looks over the probe's tip into the surgical field. The camera's video stream is augmented with coregistered, multimodality 3D graphics and landmarks obtained during neurosurgical planning with 3D workstations. The handheld probe functions as a navigation device to view and point and as an interaction device to adjust the 3D graphics. We tested the system's accuracy in the laboratory and evaluated it intraoperatively with a series of tumor and vascular cases. RESULTS: DEX-Ray provided accurate and real-time video-based augmented reality display. The system could be seamlessly integrated into the surgical workflow. The see-through effect revealing 3D information below the surgically exposed surface proved to be of significant value, especially during the macroscopic phase of an operation, providing easily understandable structural navigational information. Navigation in deep and narrow surgical corridors was limited by the camera resolution and light sensitivity. CONCLUSION: The system was perceived as an improved navigational experience because the augmented see-through effect allowed direct understanding of the surgical anatomy beyond the visible surface and direct guidance toward surgical targets.},
author = {Kockro, Ralf A. and Tsai, Yeo Tseng and Ng, Ivan and Hwang, Peter and Zhu, Chuangui and Agusanto, Kusuma and Hong, Liang Xiao and Serra, Luis},
doi = {10.1227/01.NEU.0000349918.36700.1C},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/DEX-Ray{\_}Augmented{\_}reality{\_}neurosurgical{\_}navigation.pdf:pdf},
isbn = {1524-4040 (Electronic)$\backslash$r0148-396X (Linking)},
issn = {0148396X},
journal = {Neurosurgery},
keywords = {3-Dimensional imaging,Augmented reality,Neurosurgical navigation,Virtual reality},
number = {4},
pages = {795--807},
pmid = {19834386},
title = {{DEX-Ray: Augmented reality neurosurgical navigation with a handheld video probe}},
volume = {65},
year = {2009}
}
@article{Fischer2004,
abstract = {The support of surgical interventions has long been in the focus of application-oriented augmented reality research. Modern methods of surgery, like minimally-invasive procedures, can benefit from the additional information visualization provided by augmented reality. The usability of medical augmented reality depends on a rendering scheme for virtual objects designed to generate easily and quickly understandable augmented views. One important factor for providing such an accessible reality augmentation is the correct handling of the occlusion of virtual objects by real scene elements. The usually large volumetric datasets used in medicine are ill-suited for use as phantom models for static occlusion handling. We present a simple and fast preprocessing pipeline for medical volume datasets which extracts their visual hull volume. The resulting, significantly simplified visual hull iso-surface is used for real-time static occlusion handling in our AR system, which is based on off-the-shelf medical equipment.},
author = {Fischer, Jan and Bartz, Dirk and Stra{\ss}er, Wolfgang},
doi = {10.1145/1077534.1077570},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Image{\_}Recognition/10.1.1.59.2889.pdf:pdf},
isbn = {1581139071},
journal = {Proceedings of the ACM symposium on Virtual reality software and technology - VRST '04},
keywords = {augmented reality,occlusion handling,visual,volume data},
pages = {174--177},
title = {{Occlusion handling for medical augmented reality using a volumetric phantom model}},
year = {2004}
}
@article{Jones2008,
author = {Jones, J Adam and Ii, J Edward Swan and Kolstad, Eric and Ellis, Stephen R},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Image{\_}Recognition/Depth{\_}Perception/2008{\_}Jones-etal{\_}AR-Depth-Perception{\_}IEEE-VR.pdf:pdf},
isbn = {9781595939814},
keywords = {augmented reality,depth perception,virtual reality},
number = {212},
pages = {9--14},
title = {{The Effects of Virtual Reality , Augmented Reality , and Motion Parallax on Egocentric Depth Perception}},
url = {http://dl.acm.org/citation.cfm?doid=1394281.1394283},
volume = {1},
year = {2008}
}
@article{Nilsen2005,
abstract = {By introducing virtual content onto a regular tabletop, augmented reality (AR) offers a compelling display medium for enhancing tabletop games. Coupled with tangible user interfaces, it constitutes a truly novel environment for games. Our research investigates the possibilities and constraints for succesful game design, starting with principles drawn fom tabletop and conventional computer games, and moving towards designs unique to AR. Our first major game, AR Tankwar, was recently demonstrated at GenCon Indy 2005 to over 300 players and many more spectators. This paper presents a short overview of AR Tankwar along with results from questionnaires and player discussion at the convention.},
author = {Nilsen, T},
doi = {10.1145/1152399.1152445},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Games/Nilsen-ICAT-TankwarGenCon.pdf:pdf},
isbn = {0-473-10657-4},
journal = {Proceedings of the 2005 international conference on Augmented tele-existence},
pages = {243--244},
title = {{Tankwar: Ar games at gencon indy 2005}},
volume = {157},
year = {2005}
}
@article{Roo2016,
abstract = {Moving from flat screens into the physical world is an ongoing trend of HCI. Spatial augmented reality (SAR) augments the real world using projectors. It is used in multiple disciplines, especially in design and creative environments such as prototyping and artistic installations. Still, interaction with SAR is little explored. The objective of this thesis is to study SAR and its rich space for interaction, and find novel techniques to take advantage of it. First, previous work on SAR must be studied, then experimental prototypes have to be implemented and evaluated. This paper presents a brief introduction to technical aspects that need to be addressed, in addition to a study of the design spaces related to SAR. A first proof of concept is provided: using augmented objects in front of the screen, as part of a standard desktop environment work-flow. Future work will involve iteratively creating prototypes exploring the identified strengths and weaknesses of SAR, analyzing the viability of such prototypes via dedicated user studies.},
author = {Roo, Joan Sol and Hachet, Martin},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Input/Interacting{\_}with{\_}spatially{\_}augmented{\_}rea.pdf:pdf},
keywords = {Interaction techniques,Mixed reality,Spatial Augmented Reality,Tangible User Interfaces.},
title = {{Interacting with Spatial Augmented Reality}},
url = {https://hal.archives-ouvertes.fr/hal-01284005},
year = {2016}
}
@article{Rodas2014,
abstract = {The growing use of image-guided minimally-invasive surgical procedures is confronting clinicians and surgical staff with new radiation exposure risks from X-ray imaging devices. The accurate estimation of intra-operative radiation exposure can increase staff awareness of radiation exposure risks and enable the implementation of well-adapted safety measures. The current surgical practice of wearing a single dosimeter at chest level to measure radiation exposure does not provide a sufficiently accurate estimation of radiation absorption throughout the body. In this paper, we propose an approach that combines data from wireless dosimeters with the simulation of radiation propagation in order to provide a global radiation risk map in the area near the X-ray device. We use a multi-camera RGBD system to obtain a 3D point cloud reconstruction of the room. The positions of the table, C-arm and clinician are then used 1) to simulate the propagation of radiation in a real-world setup and 2) to overlay the resulting 3D risk-map onto the scene in an augmented reality manner. By using real-time wireless dosimeters in our system, we can both calibrate the simulation and validate its accuracy at specific locations in real-time. We demonstrate our system in an operating room equipped with a robotised X-ray imaging device and validate the radiation simulation on several X-ray acquisition setups. (9 References)},
author = {{Loy Rodas}, Nicolas and Padoy, Nicolas},
doi = {10.1007/978-3-319-10404-1_52},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/paper1077.pdf:pdf},
isbn = {9783319104034},
issn = {16113349},
journal = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
keywords = {RGBD cameras,Surgical workflow analysis,augmented reality,hybrid surgery,radiation monitoring},
pages = {415--422},
pmid = {25333145},
title = {{3D global estimation and augmented reality visualization of intra-operative X-ray dose}},
year = {2014}
}
@article{Jeon2010,
abstract = {Haptic augmented reality (AR) allows to modulate the haptic properties of a real object by providing virtual haptic feedback. We previously developed a haptic AR system wherein the stiffness of a real object can be augmented with the aid of a haptic interface. To demonstrate its potential, this paper presents a case study for medical training of breast cancer palpation. A real breast model made of soft silicone is augmented with a virtual tumor rendered inside. Haptic stimuli for the virtual tumor are generated based on a contact dynamics model identified via real measurements, without the need of geometric information on the breast. A subjective evaluation confirmed the realism and fidelity of our palpation system.},
author = {Jeon, Seokhee and Knoerlein, Benjamin and Harders, Matthias and Choi, Seungmoon},
doi = {10.1109/ISMAR.2010.5643585},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/05643585.pdf:pdf},
isbn = {9781424493449},
journal = {9th IEEE International Symposium on Mixed and Augmented Reality 2010: Science and Technology, ISMAR 2010 - Proceedings},
keywords = {H.5.1 [information interfaces and presentation]: m,H.5.2 [information interfaces and presentation]: u,and virtual realities,augmented},
pages = {237--238},
title = {{Haptic simulation of breast cancer palpation: A case study of haptic augmented reality}},
year = {2010}
}
@article{Oda2008,
abstract = {Augmented reality (AR) makes it possible to create games in which virtual objects are overlaid on the real world, and real ob- jects are tracked and used to control virtual ones. We describe the development of an AR racing game created by modifying an ex- isting racing game, using an AR infrastructure that we developed for use with the XNA game development platform. In our game, the driver wears a tracked video see-through head-worn display, and controls the car with a passive tangible controller. Other players can participate by manipulating waypoints that the car must pass and obstacles with which the car can collide. We dis- cuss our AR infrastructure, which supports the creation of AR applications and games in a managed code environment, the user interface we developed for the AR racing game, the games soft- ware and hardware architecture, and feedback and observations from early demonstrations. sentation: Multimedia Information SystemsArtificial, aug- mented, and virtual realities; H.5.2 Information Interfaces and Presentation: User InterfacesInput devices and strategies, interaction styles; K.8.0 Personal Computing: General Games General Terms Human Factors, Design, Experimentation Keywords Augmented Reality, Mixed Reality, Gaming, Virtual Reality, Wearable Computer, Tangible Interaction, XNA},
author = {Oda, Ohan and Lister, Levi and White, Sean and Feiner, Steven},
doi = {10.4108/ICST.INTETAIN2008.2472},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Games/oda08.pdf:pdf},
isbn = {9789639799134},
journal = {Proceedings of the 2nd International Conference on INtelligent TEchnologies for interactive enterTAINment},
keywords = {augmented reality,gaming,mixed reality,tangible interaction,virtual reality,wearable computer,xna},
pages = {1--8},
title = {{Developing an Augmented Reality Racing Game}},
year = {2008}
}
@article{Patel2008,
abstract = {The complexity of current spinal surgery techniques and instrumentation has led to the development of image guidance technologies. Image guidance involves a series of components, notably, a system for image acquisition and a computerized system for image processing and user interface. This article reviews the options that currently exist for image guidance in spine surgery and the literature on cliincal applications of image-guided techniques in spine surgery. ?? 2008 Elsevier Inc. All rights reserved.},
author = {Patel, Alpesh A. and Whang, Peter G. and Vaccaro, Alexander R.},
doi = {10.1053/j.semss.2008.06.005},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/1-s2.0-S1040738308000488-main.pdf:pdf},
issn = {10407383},
journal = {Seminars in Spine Surgery},
keywords = {computer-guided surgery,image-guided surgery,spinal instrumentation,spine surgery},
number = {3},
pages = {186--194},
title = {{Overview of computer-assisted image-guided surgery of the spine}},
volume = {20},
year = {2008}
}
@article{Blum2012,
abstract = {The mirracle system extends the concept of an Augmented Reality (AR) magic mirror to the visualization of human anatomy on the body of the user. Using a medical volume renderer a CT dataset is augmented onto the user. By a slice based user interface, slice from the CT and an additional photographic dataset can be selected.},
author = {Blum, Tobias and Kleeberger, Valerie and Bichlmeier, Christoph and Navab, Nassir},
doi = {10.1109/VR.2012.6180934},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/06180934.pdf:pdf},
isbn = {9781467312462},
journal = {Proceedings - IEEE Virtual Reality},
keywords = {H.5.1 [Information Interfaces and Presentation]: A,and virtual realities,augmented},
pages = {169--170},
title = {{Mirracle: Augmented reality in-situ visualization of human anatomy using a magic mirror}},
year = {2012}
}
@article{Piekarski2001,
abstract = {This paper presents a summary of a new software architecture we have developed, known as Tinmith-evo5, which is designed as one possible methodology for writing complex AR applications. While software for 2D environments is very mature, in the 3D case there is still missing software support that we are attempting to address.},
author = {Piekarski, Wayne and Thomas, Bruce H},
doi = {10.1109/ISAR.2001.970530},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Platform/HMD/piekarski-tr-arch-2002.pdf:pdf},
isbn = {0-7695-1375-1},
journal = {Proceedings of the IEEE and ACM International Symposium on Augmented Reality},
keywords = {3d building construction example,3d user interfaces,augmented reality,awareness gadgets,demonstrating the,figure 1 - tinmith-metro,interface menu and situational,software architecture,this paper,tinmith-evo5 architecture described in,virtual reality,with user},
pages = {177--178},
title = {{Tinmith-evo5 - An architecture for supporting mobile augmented reality environments}},
year = {2001}
}
@article{Broll2006,
author = {Broll, Wolfgang and Ohlenburg, Jan and Lindt, Irma and Herbst, Iris and Braun, Anne-kathrin and Augustin, Sankt},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Games/10.1.1.94.3288.pdf:pdf},
isbn = {1595935894},
pages = {1--12},
title = {{Meeting Technology Challenges of Pervasive Augmented Reality Games}},
year = {2006}
}
@article{Schmalstieg2005,
abstract = {As a consequence of technical difficulties such as unreliable tracking, many AR applications get stuck in the "how to implement" phase rather than progressing to the "what to show" phase driven by information visualization needs rather than basic technology. In contrast, most of today's computer games are set in a fairly realistic 3D environment, and unlike many AR applications, game interfaces undergo extensive usability testing. This creates the interesting situation that games can be perfect simulators of AR applications, because they are able to show perfectly registered "simulated AR" overlays on top of a real-time environment. This work examines how some visualization and interaction techniques used in games can be useful for real AR applications.},
author = {Schmalstieg, Dieter},
doi = {10.1109/ISMAR.2005.17},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Games/01544681.pdf:pdf},
isbn = {0769524591},
journal = {Proceedings - Fourth IEEE and ACM International Symposium on Symposium on Mixed and Augmented Reality, ISMAR 2005},
pages = {176--177},
title = {{Augmented reality techniques in games}},
volume = {2005},
year = {2005}
}
@article{Jiang2004,
author = {Jiang, Bolan and Neumann, Ulrich and You, Suya},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Input/vr2004{\_}final.pdf:pdf},
journal = {In IEEE Virtual Reality 2004},
pages = {3--10},
title = {{A Robust Hybrid Tracking System for Outdoor Augmented Reality}},
year = {2004}
}
@misc{Bimber2001,
abstract = {A prototype of an optical extension for table-like rear-projection systems is described. A large, half-silvered mirror beam splitter is used as the optical combiner to unify a virtual and a real workbench. The virtual workbench has been enabled to display computer graphics beyond its projection boundaries and to combine virtual environments with the adjacent real world. A variety of techniques are described and referred to that allow indirect interaction with virtual objects through the mirror. Furthermore, the optical distortion that is caused by the half-silvered mirror combiner is analyzed, and techniques are presented to compensate for this distortion.},
author = {Bimber, Oliver and Encarna{\c{c}}{\~{a}}o, L M and Branco, P},
booktitle = {Presence: Teleoperators and Virtual Environments},
doi = {10.1162/105474601753272862},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Platform/SAR/US20030085866.pdf:pdf},
issn = {1054-7460},
number = {6},
pages = {613--631},
publisher = {Google Patents},
title = {{The Extended Virtual Table: An Optical Extension for Table-Like Projection Systems}},
volume = {10},
year = {2001}
}
@article{Wendler2007,
abstract = {Nuclear medicine imaging modalities assist commonly in surgical guidance given their functional nature. However, when used in the operating room they present limitations. Pre-operative tomographic 3D imaging can only serve as a vague guidance intra-operatively, due to movement, deformation and changes in anatomy since the time of imaging, while standard intra-operative nuclear measurements are limited to 1D or (in some cases) 2D images with no depth information. To resolve this problem we propose the synchronized acquisition of position, orientation and readings of gamma probes intra-operatively to reconstruct a 3D activity volume. In contrast to conventional emission tomography, here, in a first proof-of-concept, the reconstruction succeeds without requiring symmetry in the positions and angles of acquisition, which allows greater flexibility. We present our results in phantom experiments for sentinel node lymph node localization. The results indicate that 3D intra-operative nuclear images can be generated in such a setup up to an accuracy equivalent to conventional SPECT systems. This technology has the potential to advance standard procedures towards intra-operative 3D nuclear imaging and offers a novel approach for robust and precise localization of functional information to facilitate less invasive, image-guided surgery.},
author = {Wendler, Thomas and Hartl, Alexander and Lasser, Tobias and Traub, Joerg and Daghighian, Farhad and Ziegler, Sibylle I and Navab, Nassir},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/47920909.pdf:pdf},
isbn = {978-3-540-75758-0},
issn = {03029743},
journal = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
pages = {909--917},
pmid = {18044655},
title = {{Towards intra-operative 3D nuclear imaging: reconstruction of 3D radioactive distributions using tracked gamma probes}},
volume = {10},
year = {2007}
}
@article{Kato1999,
abstract = {We describe an augmented reality conferencing system which uses the overlay of virtual images on the real world. Remote collaborators are represented on Virtual Monitors which can be freely positioned about a user in space. Users can collaboratively view and interact with virtual objects using a shared virtual whiteboard. This is possible through precise virtual image registration using fast and accurate computer vision techniques and HMD calibration. We propose a method for tracking fiducial markers and a calibration method for optical see-through HMD based on the marker tracking.},
author = {Kato, Hirokazu and Billinghurst, Mark},
doi = {10.1109/IWAR.1999.803809},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Platform/HMD/IWAR99{\_}AR{\_}Toolkit.pdf:pdf},
isbn = {0-7695-0359-4},
issn = {16136829},
journal = {Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality (IWAR'99)},
number = {February},
pages = {85--94},
title = {{Marker tracking and HMD calibration for a video-based augmented reality conferencing system}},
year = {1999}
}
@article{Yi-bo2008,
abstract = {The registration in augmented reality is a process which merges virtual objects generated by computer with real world image caught by camera. This paper describes the tracker-based registration and the knowledge-based registration, and analyzes the process and current approaches of computer vision-based registration. In consideration of current research challenges, the outdoor registration technology in augmented reality is discussed in detail, and some typical applications are introduced. Then, on the basis of the development background of "UAV teleoperation and simulation system", an augmented reality method for UAV teleoperation based on Vega is illustrated. At last, it indicates that correct 3D registration in outdoor complex environment with large area and the application in teleoperation of robot or UAV is an important developing tendency in the research field of registration technology in augmented reality.},
author = {Yi-bo, Li and Shao-peng, Kang and Zhi-hua, Qiao and Qiong, Zhu},
doi = {10.1109/ISCID.2008.120},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Head{\_}Tracking/04725459.pdf:pdf},
isbn = {978-0-7695-3311-7},
journal = {2008 International Symposium on Computational Intelligence and Design},
keywords = {Vega,application,augmented reality,registration technology},
pages = {69--74},
title = {{Development actuality and application of registration technology in augmented reality}},
volume = {2},
year = {2008}
}
@article{Barrie2009,
abstract = {This paper describes the prototype implementation of a pervasive,$\backslash$nwearable augmented reality (AR) system based on a full body-motion-capture$\backslash$nsystem using low-power wireless sensors. The system uses body motion$\backslash$nto visualize and interact with virtual objects populating AR settings.$\backslash$nBody motion is used to implement a whole body gesture-driven interface$\backslash$nto manipulate the virtual objects. Gestures are mapped to correspondent$\backslash$nbehaviors for virtual objects, such as controlling the playback and$\backslash$nvolume of virtual audio players or displaying a virtual object's$\backslash$nmetadata.},
author = {Barrie, Peter and Komninos, Andreas and Mandrychenko, Oleksii},
doi = {10.1145/1710035.1710096},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Input/acmMobility09.pdf:pdf},
isbn = {9781605585369},
journal = {Mobility '09 Proceedings of the 6th International Conference on Mobile Technology, Application {\&} Systems},
keywords = {augmented reality,body area networks,computing,hci,mobile,orientation,upper body gesture recognition,user interaction,wireless sensor networks},
pages = {1--4},
title = {{A pervasive gesture-driven augmented reality prototype using wireless sensor body area networks}},
year = {2009}
}
@article{Pessoa2010,
abstract = {This paper presents a solution for the photorealistic rendering of synthetic objects into dynamic real scenes, in Augmented Reality applications. In order to achieve this goal, an Image Based Lighting approach is used, where environment maps with different levels of glossiness are generated for each virtual object in the scene at every frame. Due to this, illumination effects, such as color bleeding and specular reflections, can be simulated for virtual objects in a consistent way. A unifying sampling method for the spherical harmonics transformation pass is also used. It is independent of map format and does not need to apply different weights for each sample. The developed technique is combined with an extended version of Lafortune Spatial BRDF, featuring Fresnel effect and an innovative tangent rotation parameterization. The solution is evaluated in various Augmented Reality case studies, where other features like shadowing and lens effects are also exploited.},
author = {Pessoa, Saulo and Moura, Guilherme and Lima, Jo??o and Teichrieb, Veronica and Kelner, Judith},
doi = {10.1109/VR.2010.5444836},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Image{\_}Recognition/Illumination/05444836.pdf:pdf},
isbn = {9781424462582},
issn = {1087-8270},
journal = {Proceedings - IEEE Virtual Reality},
keywords = {H.5.1 [information interfaces and presentation]: m,I.3.7 [computer graphics]: three-dimensional graph},
pages = {3--10},
title = {{Photorealistic rendering for augmented reality: A global illumination and BRDF solution}},
url = {http://ieeexplore.ieee.org/document/5444836/},
year = {2010}
}
@article{Merloz2007,
author = {Merloz, Philippe and Troccaz, Jocelyne and Vouaillat, Herv{\{}$\backslash$'e{\}} and Vasile, Christian and Tonetti, J{\{}$\backslash$'e{\}}r{\{}$\backslash${\^{}}o{\}}me and Eid, Ahmad and Plaweski, St{\{}$\backslash$'e{\}}phane},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/0711.4516.pdf:pdf},
journal = {Proceedings of the Institution of Mechanical Engineers, Part H: Journal of Engineering in Medicine},
number = {1},
pages = {813-- 820},
title = {{Fluoroscopy-based navigation system in orthopaedic surgery}},
volume = {221},
year = {2007}
}
@article{Schall2009,
abstract = {In this paper, we present an Augmented Reality (AR) system for aiding field workers of utility companies in outdoor tasks such as maintenance, planning or surveying of underground infrastructure. Our work addresses these issues using spatial interaction and visualization techniques for mobile AR applications and as well as for a new mobile device design. We also present results from evaluations of the prototype application for underground infrastructure spanning various user groups. Our application has been driven by feedback from industrial collaborators in the utility sector, and includes a translation tool for automatically importing data from utility company databases of underground assets.},
author = {Schall, Gerhard and Mendez, Erick and Kruijff, Ernst and Veas, Eduardo and Junghanns, Sebastian and Reitinger, Bernhard and Schmalstieg, Dieter},
doi = {10.1007/s00779-008-0204-5},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Platform/HHD/art{\%}3A10.1007{\%}2Fs00779-008-0204-5.pdf:pdf},
isbn = {9781420051209},
issn = {16174909},
journal = {Personal and Ubiquitous Computing},
keywords = {Geospatial modeling,Handheld Augmented Reality,Mobile spatial interaction,Online 3D reconstruction},
number = {4},
pages = {281--291},
title = {{Handheld Augmented Reality for underground infrastructure visualization}},
url = {http://link.springer.com/article/10.1007/s00779-008-0204-5},
volume = {13},
year = {2009}
}
@article{Azuma2001,
abstract = {In 1997, Azuma published a survey on augmented reality (AR). Our goal is to complement, rather than replace, the original survey by presenting representative examples of the new advances. We refer one to the original survey for descriptions of potential applications (such as medical visualization, maintenance and repair of complex equipment, annotation, and path planning); summaries of AR system characteristics (such as the advantages and disadvantages of optical and video approaches to blending virtual and real, problems in display focus and contrast, and system portability); and an introduction to the crucial problem of registration, including sources of registration error and error-reduction strategies},
author = {Azuma, Ronald and Baillot, Yohan and Behringer, Reinhold and Feiner, Steven and Julier, Simon and MacIntyre, Blair},
doi = {10.1109/38.963459},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Surveys/Recent{\_}Advances{\_}in{\_}Augmented{\_}Reality{\_}2001.pdf:pdf},
isbn = {0272-1716},
issn = {02721716},
journal = {IEEE Computer Graphics and Applications},
keywords = {environments,head-mounted displays,issues,performance,registration,see-through,tracking system,visualization},
number = {6},
pages = {34--47},
pmid = {22174708},
title = {{Recent advances in augmented reality}},
volume = {21},
year = {2001}
}
@article{Papagiannakis2008,
author = {Papagiannakis, George and Singh, Gurminder and Magnenat-Thalmann, Nadia},
doi = {10.1002/cav.221},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Surveys/Singh{\_}d912f5075af50e0812{\_}2008.pdf:pdf},
journal = {Computer Animation and Virtual Worlds},
keywords = {augmented-mixed reality,mobile systems,wireless networking},
number = {1},
pages = {3--22},
title = {{A survey of mobile and wireless technologies for augmented reality systems}},
volume = {19},
year = {2008}
}
@article{Hilliges2012,
abstract = {Abstract HoloDesk is an interactive system combining an optical see through display and Kinect camera to create the illusion that users are directly interacting with 3D graphics. A virtual image of a 3D scene is rendered through a half silvered mirror and spatially aligned ... $\backslash$n},
author = {Hilliges, O and Kim and Izadi, S and Weiss, M and Wilson, A},
doi = {10.1145/2207676.2208405},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Input/p2421-hilliges.pdf:pdf},
isbn = {9781450310154},
journal = {Proceedings of CHI 2012},
pages = {2421--2430},
title = {{Holodesk: Direct 3d interactions with a situated see-through display}},
year = {2012}
}
@article{Kiyokawa2003,
abstract = {An ideal augmented reality (AR) display for multi-user co-located collaboration should have following three features: 1) any virtual object should be able to be shown at any arbitrary position, e.g. a user can see a virtual object in front of other users' faces. 2) Correct occlusion of virtual and real objects should be supported. 3) The real world should be naturally and clearly visible, which is important for face-to-face conversation. We have been developing an optical see-through display, ELMO (Enhanced see-through display using an LCD panel for Mutual Occlusion), that satisfies these three requirements. While previous prototype systems were not practical due to their size and weight, we have come up with an improved optics design which has reduced size and is lightweight enough to wear. In this paper, the characteristics of typical multi-user three-dimensional displays are summarized and the design details of the latest optics are then described. Finally, a collaborative AR application employing the new display and its user experience are explained.},
author = {Kiyokawa, Kiyoshi and Billinghurst, Mark and Campbell, Bruce and Woods, Eric},
doi = {10.1109/ISMAR.2003.1240696},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Platform/HMD/download.pdf:pdf},
isbn = {0769520065},
journal = {Proceedings of the 2nd IEEE/ACM International Symposium on Mixed and Augmented Reality},
number = {November 2016},
pages = {133--141},
title = {{An occlusion capable optical see-through head mount display for supporting co-located collaboration}},
year = {2003}
}
@article{Ohshima1998,
abstract = {Introduces a collaborative augmented reality (AR) system for real-time interactive operations. AR enables us to enhance physical space with computer-generated virtual space. In addition, collaborative AR allows multiple participants to simultaneously share the physical space surrounding them and a virtual space that is visually registered with the physical one. They can also communicate with each other through the mixed space. This paper describes the AR2Hockey (Augmented Reality AiR Hockey) system, where players can share a physical game field and mallets, and a virtual puck to play an air-hockey game, as a case study of a collaborative AR system. Since real-time accurate registration between both spaces and players is crucial for the collaboration, a video-rate registration algorithm is implemented with magnetic head-trackers and video cameras attached to optical see-through head-mounted displays (HMDs). The configuration of the system and the details of the registration are described. Our experimental collaborative AR system achieves higher interactivity than a totally immersive collaborative VR system.},
author = {Ohshima, Toshikazu and Satoh, Kiyohide and Yamamoto, Hiroyuki and Tamura, Hideyuki},
doi = {10.1109/VRAIS.1998.658505},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Games/2ed16ee1f845b3048687ebe027053f38702e.pdf:pdf},
isbn = {0-8186-8362-7},
journal = {Virtual Reality Annual International Symposium, 1998. Proceedings., IEEE 1998},
keywords = {augmented reality,collaboration,registration},
pages = {268--275},
title = {{AR2Hockey: A case study of collaborative augmented reality}},
year = {1998}
}
@article{Azuma1997a,
abstract = {This paper surveys the field of Augmented Reality, in which 3-D virtual $\backslash$nobjects are integrated into a 3-D real environment in real time. It describes the $\backslash$nmedical, manufacturing, visualization, path planning, entertainment and military $\backslash$napplications that have been explored. This paper describes the characteristics of $\backslash$nAugmented Reality systems, including a detailed discussion of the tradeoffs between $\backslash$noptical and video blending approaches. Registration and sensing errors are two of the $\backslash$nbiggest problems in building effective Augmented Reality systems, so this paper $\backslash$nsummarizes current efforts to overcome these problems. Future directions and areas $\backslash$nrequiring further research are discussed. This survey provides a starting point for $\backslash$nanyone interested in researching or using Augmented Reality.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Azuma, Ronald T},
doi = {10.1.1.30.4999},
eprint = {arXiv:1011.1669v3},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Surveys/Visualizacao-Gr-LuisPattam-paperdeapoio-1.pdf:pdf},
isbn = {1054-7460},
issn = {10547460},
journal = {Presence: Teleoperators and Virtual Environments},
number = {4},
pages = {355--385},
pmid = {9708264137},
title = {{A survey of augmented reality}},
volume = {6},
year = {1997}
}
@article{Piekarski2003,
abstract = {This paper presents a new software architecture for 3D mixed reality applications, named Tinmith-evo5. Currently there are a limited number of existing toolkits for the development of 3D mixed reality applications, each optimized for particular feature but at the detriment of others. Complex interactive user interfaces and applications require extensive supporting infrastructure, and can be hampered by inadequate support. The Tinmith-evo5 architecture is optimised to develop mobile augmented reality and other interactive 3D applications on portable platforms with limited resources. This architecture is implemented in C++ with an object-oriented data flow design, an object store based on the Unix file system model, and uses other ideas from existing previous work.},
author = {Piekarski, W. and Thomas, B.H.},
doi = {10.1109/ISMAR.2003.1240708},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/download.pdf:pdf},
isbn = {0-7695-2006-5},
journal = {The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.},
keywords = {3D mixed reality applications,Application software,Augmented reality,C++ language,Computer architecture,Hardware,Programming,Rendering (computer graphics),Software architecture,Tinmith-evo5,Unix,Unix file system,User interfaces,Virtual reality,Wearable computers,augmented reality,data flow computing,data flow design,file system model,infrastructure support,interactive 3D applications,interactive user interfaces,mobile augmented reality,object-oriented data flow,object-oriented software,optimisation,portable platforms,software architecture,software development tools,software tools,user interfaces},
number = {C},
pages = {247--256},
title = {{An object-oriented software architecture for 3D mixed reality applications}},
year = {2003}
}
@article{Blum2009,
abstract = {Ultrasound (US) is a medical imaging modality which is extremely difficult to learn as it is user-dependent, has low image quality and requires much knowledge about US physics and human anatomy. For training US we propose an Augmented Reality (AR) ultrasound simulator where the US slice is simulated from a CT volume. The location of the US slice inside the body is visualized using contextual in-situ techniques. We also propose advanced methods how to use an AR simulator for training.},
author = {Blum, Tobias and Heining, Sandro Michael and Kutter, Oliver and Navab, Nassir},
doi = {10.1109/ISMAR.2009.5336476},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/05336476.pdf:pdf},
isbn = {978-1-4244-5390-0},
journal = {2009 8th IEEE International Symposium on Mixed and Augmented Reality},
keywords = {Augmented reality,Biological system modeling,Biomedical imaging,CT volume,Computed tomography,H.5.1 [Information Interfaces and Presentation]: A,Human anatomy,Image quality,J.3 [Life and Medical Sciences]: —,Medical simulation,Physics,US physics-and-human anatomy,US slice,Ultrasonic imaging,Visualization,augmented reality ultrasound simulator,biomedical ultrasonics,computer based training,contextual in-situ visualization,data visualisation,medical image processing,medical imaging,teaching,training method},
pages = {177--178},
title = {{Advanced training methods using an Augmented Reality ultrasound simulator}},
year = {2009}
}
@article{Bajura1992,
abstract = {We describe initial results which show “live” ultrasound echography data visualized within a pregnant human subject. The visualization is achieved by using a small video camera mounted in front of a conventional head-mounted display worn by an observer. The camera's video images are composite with computer-generated ones that contain one or more 2D ultrasound images properly transformed to the observer's current viewing position. As the observer walks around the subject. the ultrasound images appear stationary in 3-space within the subject. This kind of enhancement of the observer's vision may have many other applications, e.g., image guided surgical procedures and on location 3D interactive architecture preview.},
author = {Bajura, Michael and Fuchs, Henry and Ohbuchi, Ryutarou},
doi = {10.1145/142920.134061},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/71b6b1c80a980d328b423e34af27951bd180.pdf:pdf},
isbn = {0-89791-479-1},
issn = {00978930},
journal = {Computer Graphics},
number = {2},
pages = {203--210},
title = {{Merging Virtual Objects with the Real World : Seeing Ultrasound Imagery within the Patient}},
volume = {26},
year = {1992}
}
@article{Mavrogenis2013,
abstract = {Computer-assisted navigation has a role in some orthopedic procedures. It allows the surgeons to obtain real-time feedback and offers the potential to decrease intra-operative errors and optimize the surgical result. Computer-assisted navigation systems can be active or passive. Active navigation systems can either perform surgical tasks or prohibit the surgeon from moving past a predefined zone. Passive navigation systems provide intraoperative information, which is displayed on a monitor, but the surgeon is free to make any decisions he or she deems necessary. This article reviews the available types of computer-assisted navigation, summarizes the clinical applications and reviews the results of related series using navigation, and informs surgeons of the disadvantages and pitfalls of computer-assisted navigation in orthopedic surgery.},
author = {Mavrogenis, Andreas F and Savvidou, Olga D and Mimidis, George and Papanastasiou, John and Koulalis, Dimitrios and Demertzis, Nikolaos and Papagelopoulos, Panayiotis J},
doi = {10.3928/01477447-20130724-10},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/computer-assisted-navigation-in-orthopedic-surgery.pdf:pdf},
issn = {1938-2367},
journal = {Orthopedics},
keywords = {Computer-Assisted,Computer-Assisted: methods,Humans,Orthopedic Procedures,Orthopedic Procedures: methods,Robotics,Robotics: methods,Surgery,Telemedicine,Telemedicine: methods,User-Computer Interface},
number = {8},
pages = {631--42},
pmid = {23937743},
title = {{Computer-assisted navigation in orthopedic surgery}},
volume = {36},
year = {2013}
}
@article{Stolka2011,
abstract = {Handheld ultrasound is useful for intra-operative imaging, but requires$\backslash$nadditional tracking hardware to be useful in navigated intervention$\backslash$nsettings, such as biopsies, ablation therapy, injections etc. Unlike$\backslash$ncommon probe-and-needle tracking approaches involving global or local$\backslash$ntracking, we propose to use a bracket with a combination of very$\backslash$nlow-cost local sensors - cameras with projectors, optical mice and$\backslash$naccelerometers - to reconstruct patient surfaces, needle poses, and the$\backslash$nprobe trajectory with multiple degrees of freedom, but no global$\backslash$ntracking overhead. We report our experiences from a first series of$\backslash$nbenchtop and in-vivo human volunteer experiments.},
author = {Stolka, Philipp J. and Wang, Xiang Linda and Hager, Gregory D. and Boctor, Emad M.},
doi = {10.1117/12.878901},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/title{\_}Navigation{\_}with{\_}local{\_}sensors{\_}in.pdf:pdf},
isbn = {1410516539},
issn = {16057422},
journal = {Imaging},
keywords = {3d ultrasound,guidance,handheld,local sensors,navigation,needle interventions,vision},
title = {{Navigation with local sensors in handheld 3D ultrasound: initial in-vivo experience}},
year = {2011}
}
@article{Lee2007,
author = {Lee, Taehee and Tobias, H},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Input/wwd{\_}090803085913.pdf:pdf},
isbn = {1424414539},
title = {{Handy AR : Markerless Inspection of Augmented Reality Objects Using Fingertip Tracking}},
year = {2007}
}
@article{Rolland2001,
abstract = {Tracking for virtual environments is necessary to record the position and the orientation of real objects in physical space and to allow spatial consistency between real and virtual objects. This paper presents a top-down classification of tracking technologies aimed more specifically at head tracking, organized in accordance with their physical principles of operation. Six main principles were identified: time of flight (TOF), spatial scan, inertial sensing, mechanical linkages, phase-difference sensing, and direct-field sensing. We briefly describe each physical principle and present implementations of that principle. Advantages and limitations of these implementations are discussed and summarized in tabular form. A few hybrid technologies are then presented and general considerations of tracking technology are discussed.},
author = {Rolland, Jannick P. and Baillot, Yohan and Goon, Alexei A.},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Head{\_}Tracking/1522.pdf:pdf},
journal = {Fundamentals of wearable computers and augmented reality},
number = {1},
pages = {67--112},
title = {{A survey of tracking technology for virtual environments}},
volume = {1},
year = {2001}
}
@article{Choi2009,
abstract = {For interactive augmented reality, vision-based and hand-gesture-based interface are most desirable due to being natural and human-friendly. However, detecting hands and recognizing hand gestures in cluttered background are still challenging. Especially, if the background includes a large skin-colored region, the problem becomes more difficult. In this paper, we focus on detecting a hand reliably and propose an effective method. Our method is basically based on the assumption that a hand-forearm region (including a hand and part of a forearm) has different brightness from other skin-colored regions. Specifically, we first segment the hand-forearm region from other skin-colored regions based on the brightness difference which is represented by edges in this paper. Then, we extract the hand region from the hand-forearm region by detecting a feature point which indicates the wrist. Finally, we extract the hand by using the brightness-based segmentation which is slightly different from the hand-forearm region detection. We verify the effectiveness of our method by implementing a simple hand gesture interface based on our method and applying it to augmented reality applications.},
author = {Choi, Junyeong and Seo, Byung-kuk and Park, Jong-II},
doi = {10.1145/1670252.1670324},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Input/2f92dfb470dd8f0d664df3ddc7487e6f9d91.pdf:pdf},
isbn = {9781605589121},
journal = {Proceedings of the 8th International Conference on Virtual Reality Continuum and its Applications in Industry},
keywords = {augmented,hand detection,hand gesture recognition},
number = {212},
pages = {319--322},
title = {{Robust Hand Detection for Augmented Reality Interface}},
volume = {1},
year = {2009}
}
@article{Sutherland1968,
abstract = {The fundamental idea behind the three-dimensional display is to present the user with a perspective image which changes as he moves. The retinal image of the real objects which we see is, after all, only two-dimensional. Thus if we can place suitable two-dimensional images on the observer's retinas, we can create the illusion that he is seeing a three-dimensional object. Although stereo presentation is important to the three-dimensional illusion, it is less important than the change that takes place in the image when the observer moves his head. The image presented by the three-dimensional display must change in exactly the way that the image of a real object would change for similar motions of the user's head. Psychologists have long known that moving perspective images appear strikingly three-dimensional even without stereo presentation; the three-dimensional display described in this paper depends heavily on this "kinetic depth effect."},
author = {Sutherland, Ivan E.},
doi = {10.1145/1476589.1476686},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Platform/HMD/50720757.pdf:pdf},
isbn = {158113052X},
journal = {Proceedings of the December 9-11, 1968, fall joint computer conference, part I},
pages = {757--764},
title = {{A head-mounted three dimensional display}},
year = {1968}
}
@article{Rolland1994,
abstract = {One of the most promising and challenging future uses of head-mounted displays (HMDs) is in applications where virtual environments enhance rather than replace real environments. To obtain an enhanced view of the real environment, the user wears a see-through HMD to see 3D computergenerated objects superimposed on his/her real-world view. This see-through capability can be accomplished using either an optical or a video see-through HMD. We discuss the tradeoffs between optical and video see-through HMDs with respect to technological, perceptual, and human factors issues, and discuss our experience designing, building, using, and testing these HMDs.},
author = {Rolland, Jannick P. and Holloway, Richard and Fuchs, Henry},
doi = {10.1117/12.197322},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Platform/HMD/1505.PDF:PDF},
isbn = {0277786X},
issn = {0277786X},
journal = {Photonics for Industrial Applications},
keywords = {augmented reality,optics,see-through head-mounted displays,superimposition,video cameras},
pages = {293--307},
title = {{Comparison of optical and video see-through, head-mounted displays}},
volume = {2351},
year = {1995}
}
@article{Miyashita2008,
abstract = {Recent years have seen advances in many enabling augmented reality technologies. Furthermore, much research has been carried out on how augmented reality can be used to enhance existing applications. This paper describes our experiences with an AR-museum guide that combines some of the latest technologies. Amongst other technologies, markerless tracking, hybrid tracking, and an ultra-mobile-PC were used. Like existing audio guides, the AR-guide can be used by any museum visitor, during a six-month exhibition on Islamic art. We provide a detailed description of the museumpsilas motivation for using AR, of our experiences in developing the system, and the initial results of user surveys. Taking this information into account, we can derive possible system improvements.},
author = {Miyashita, T. and Meier, P. and Tachikawa, T. and Orlic, S. and Eble, T. and Scholz, V. and Gapel, A. and Gerl, O. and Arnaudov, S. and Lieberknecht, S.},
doi = {10.1109/ISMAR.2008.4637334},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/04637334.pdf:pdf},
isbn = {9781424428403},
journal = {Proceedings - 7th IEEE International Symposium on Mixed and Augmented Reality 2008, ISMAR 2008},
keywords = {Augmented reality,Mobile computing,Multimedia in the museu},
pages = {103--106},
title = {{An augmented reality museum guide}},
year = {2008}
}
@article{Navab1999,
abstract = {This paper presents the basic concept of CAMC and some of its applications. A CCD camera is attached to a mobile C-arm fluoroscopy X-ray system. Both optical and X-ray imaging systems are calibrated in the same coordinate system in an off-line process. The new system is able to provide X-ray and optical images simultaneously. The CAMC framework has great potential for medical augmented reality. We briefly introduce two new CAMC applications to the augmented reality research community. The first application aims at merging video images with a pre-computed tomographic reconstruction of the 3D volume of interest. This is a logical continuation of our work on 3D reconstruction using a CAMC (1999). The second approach is a totally new CAMC design where using a double mirror system and an appropriate calibration procedure the X-ray and optical images are merged in real-time. This new system enables the user to see an optical image, an X-ray image, or an augmented image where both visible and invisible are combined in real-time. The paper is organized in two independent sections describing each of the above. Experimental results are provided at the same time as the methods and apparatus are described for each section},
author = {Navab, N. and Bani-Kashemi, A. and Mitschke, M.},
doi = {10.1109/IWAR.1999.803814},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/download.pdf:pdf},
isbn = {0-7695-0359-4},
journal = {Proceedings 2nd IEEE and ACM International Workshop on Augmented Reality (IWAR'99)},
keywords = {3D volume of interest,Augmented reality,Biomedical imaging,Biomedical optical imaging,CAMC,CCD camera,CCD image sensors,Camera-Augmented Mobile C-arm,Charge coupled devices,Charge-coupled image sensors,Image reconstruction,Merging,Real time systems,Tomography,X-ray imaging,augmented reality,calibration procedure,computerised tomography,coordinate system,double mirror system,image reconstruction,medical augmented reality,medical image processing,mobile C-arm fluoroscopy X-ray system,optical imaging,real-time systems,tomographic reconstruction,video images},
number = {October 2013},
pages = {134--141},
title = {{Merging visible and invisible: two Camera-Augmented Mobile C-arm (CAMC) applications}},
year = {1999}
}
@article{Broll2008,
abstract = {Mobile augmented reality games offer a new and rich game experience allowing players to move and interact in their physical environment with 3D content. The authors review existing approaches to mobile AR games and identify two major trends: small, user-modifiable AR games and larger-scale, event-based AR games that are interwoven with their physical environment.},
author = {Broll, Wolfgang and Lindt, Irma and Herbst, Iris and Ohlenburg, Jan and Braun, Anne Kathrin and Wetzel, Richard},
doi = {10.1109/MCG.2008.85},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Games/04557954.pdf:pdf},
isbn = {0272-1716 VO - 28},
issn = {02721716},
journal = {IEEE Computer Graphics and Applications},
keywords = {3D interaction techniques,Augmented reality,Augmented reality games,Computers,Games,Hardware,Mixed reality,Mobile communication,Mobile games,Outdoor augmented reality,Three dimensional displays,Virtual reality},
number = {4},
pages = {40--48},
title = {{Toward next-gen mobile AR games}},
volume = {28},
year = {2008}
}
@article{Stauder1999,
author = {Stauder, J{\"{u}}rgen},
doi = {10.1109/6046.766735},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Image{\_}Recognition/Illumination/00766735.PDF:PDF},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Augmented reality,Lighting estimation,Maximum likelihood estimation},
month = {jun},
number = {2},
pages = {136--143},
title = {{Augmented reality with automatic illumination control incorporating ellipsoidal models}},
url = {http://ieeexplore.ieee.org/document/766735/},
volume = {1},
year = {1999}
}
@article{Kruijff2010,
author = {Kruijff, Ernst and Ii, J Edward Swan},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Image{\_}Recognition/Depth{\_}Perception/05643530.pdf:pdf},
pages = {3--12},
title = {{Perceptual Issues in Augmented Reality Revisited}},
url = {http://ieeexplore.ieee.org/document/5643530/},
year = {2010}
}
@article{State1996,
abstract = {Accurate registration between real and virtual objects is crucial for augmented reality applications. Existing tracking methods are individually inadequate: magnetic trackers are inaccurate, mechanical trackers are cumbersome, and vision-based trackers are computationally problematic. We present a hybrid tracking method that combines the accuracy of vision-based tracking with the robustness of magnetic tracking without compromising real-time performance or usability. We demonstrate excellent registration in three sample applications.},
author = {State, Andrei and Hirota, Gentaro and Chen, David T and Garrett, William F and Livingston, Mark a},
doi = {10.1145/237170.237282},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Image{\_}Recognition/Registration/state96.pdf:pdf},
isbn = {0897917464},
issn = {0097-8930},
journal = {Proceedings of the 23rd annual conference on Computer graphics and interactive techniques SIGGRAPH 96},
number = {Annual Conference Series},
pages = {429--438},
pmid = {18212804},
title = {{Superior augmented reality registration by integrating landmark tracking and magnetic tracking}},
url = {http://portal.acm.org/citation.cfm?doid=237170.237282},
volume = {30},
year = {1996}
}
@article{Vogt2006,
abstract = {Augmented Reality is an emerging technology that seeks to enhance a user's view by overlaying graphical information. We developed a prototype AR system geared for medical applications. It is built around a stereoscopic head-mounted display of the video-see-through variety. The newest generation of this prototype system exhibits high performance on a standard PC platform. Stereoscopic video images are augmented with medical graphics in real-time at 30 frames per second and with XGA (1024×768) resolution. The system provides a compelling AR perception: the graphics appears firmly anchored in the scene—there is no time lag between video and graphics or any apparent jitter of the graphics.With the head-mounted display, the user has a natural and direct access to understanding the 3D structure of the scene, based on both stereo and kinetic depth cues. In the present paper, we describe in detail the architecture and several features of the AR prototype system. Head tracking is accomplished with a single-camera system, with the dedicated tracker camera placed on the head-mounted display. This configuration is the foundation of achieving a high-accuracy graphics overlay.We are nowexploring the use of the prototype system for a variety of medical applications. This paper gives an overview over the pre-clinical tests that we have performed for interventional guidance. Overall, the feedback has been very positive and encouraging, and we are continuing to work towards realizing the clinical potential of the technology},
author = {Vogt, Sebastian and Khamene, Ali and Sauer, Frank},
doi = {10.1007/s11263-006-7938-1},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/art{\%}3A10.1007{\%}2Fs11263-006-7938-1.pdf:pdf},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {AR system architecture,Augmented reality,Computer assisted surgery,In-situ visualization,Real-time system,Single camera marker tracking},
number = {2},
pages = {179--190},
title = {{Reality augmentation for medical procedures: System architecture, single camera marker tracking, and system evaluation}},
volume = {70},
year = {2006}
}
@article{Rosenthal2002,
abstract = {We report the results of a randomized, controlled trial to$\backslash$ncompare the accuracy of standard ultrasound-guided needle biopsy to$\backslash$nbiopsies performed using a 3D Augmented Reality (AR) guidance system.$\backslash$nFifty core biopsies of breast phantoms were conducted by a$\backslash$nboard-certified radiologist, with each set of five biopsies randomly$\backslash$nassigned to one of the methods. The raw ultrasound data from each$\backslash$nbiopsy was recorded. Another board-certified radiologist, blinded to$\backslash$nthe actual biopsy guidance mechanism, evaluated the ultrasound recordings$\backslash$nand determined the distance of the biopsy from the ideal position. A$\backslash$nrepeated measures analysis of variance indicated that the$\backslash$nhead-mounted display method led to a statistically significantly$\backslash$nsmaller mean deviation from the desired target than did the CRT display$\backslash$nmethod. (2.48mm for control versus 1.62mm for augmented reality,$\backslash$np {\textless} 0.02). This result suggests that AR systems can offer improved$\backslash$naccuracy over traditional biopsy guidance methods.},
author = {Rosenthal, Michael and State, Andrei and Lee, Joohi and Hirota, Gentaro and Ackerman, Jeremy and Keller, Kurtis and Pisano, Etta D. and Jiroutek, Michael and Muller, Keith and Fuchs, Henry},
doi = {10.1007/3-540-45468-3_29},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Medical/Rosenthal{\_}MIA{\_}2002.pdf:pdf},
isbn = {3540426973},
issn = {16113349},
journal = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
keywords = {augmented reality,breast biopsy,needle biopsy,ultrasound guidance},
pages = {240--248},
pmid = {12270235},
title = {{Augmented reality guidance for needle biopsies: A randomized, controlled trial in phantoms}},
volume = {2208},
year = {2001}
}
@article{Wagner2004,
author = {Wagner, Daniel and Pintaric, Thomas and Schmalstieg, Dieter},
journal = {ACM SIGGRAPH 2004 Emerging technologies},
pages = {12},
title = {{The invisible train: A collaborative handheld augmented reality demonstrator}},
year = {2004}
}
@article{Henderson2009,
abstract = {We present the design, implementation, and user testing of a prototype augmented reality application to support military mechanics conducting routine maintenance tasks inside an armored vehicle turret. Our prototype uses a tracked head-worn display to augment a mechanic's natural view with text, labels, arrows, and animated sequences designed to facilitate task comprehension, location, and execution. A within-subject controlled user study examined professional military mechanics using our system to complete 18 common tasks under field conditions. These tasks included installing and removing fasteners and indicator lights, and connecting cables, all within the cramped interior of an armored personnel carrier turret. An augmented reality condition was tested against two baseline conditions: an untracked headworn display with text and graphics and a fixed flat panel display representing an improved version of the laptop-based documentation currently employed in practice. The augmented reality condition allowed mechanics to locate tasks more quickly than when using either baseline, and in some instances, resulted in less overall head movement. A qualitative survey showed mechanics found the augmented reality condition intuitive and satisfying for the tested sequence of tasks.},
author = {Henderson, Steven J. and Feiner, Steven},
doi = {10.1109/ISMAR.2009.5336486},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Military/10.1.1.479.8523.pdf:pdf},
isbn = {9781424453900},
journal = {Science and Technology Proceedings - IEEE 2009 International Symposium on Mixed and Augmented Reality, ISMAR 2009},
keywords = {Attention,Localization augmented reality,Maintenance,Repair,Service},
pages = {135--144},
title = {{Evaluating the benefits of augmented reality for task localization in maintenance of an armored personnel carrier turret}},
year = {2009}
}
@incollection{Raskar1998,
abstract = {To create an effective illusion of virtual objects coexisting with the real world, see-through HMD-based Augmented Reality techniques supplement the user's view with images of virtual objects. We introduce here a new paradigm, Spatially Augmented Reality (SAR), where virtual objects are rendered directly within or on the user's physical space. A key benefit of SAR is that the user does not need to wear a head-mounted display. Instead, with the use of spatial displays, wide field of view and possibly high-resolution images of virtual objects can be integrated directly into the environment. For example, the virtual objects can be realized by using digital light projectors to "paint" 2D/3D imagery onto real surfaces, or by using built-in flat panel displays. In this paper we present the rendering method used in our implementation and discuss the fundamentally different visible artifacts that arise as a result of errors in tracker measurements. Finally, we speculate about how SAR techniques might be combined with see-through AR to provide an even more compelling AR experience.},
author = {Raskar, Ramesh and Welch, Greg and Fuchs, Henry},
booktitle = {First IEEE Workshop on Augmented Reality (IWAR'98)},
doi = {10.1109/TVCG.2009.209},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Platform/SAR/10.1.1.17.4260.pdf:pdf},
isbn = {1568810989},
issn = {1077-2626},
pages = {11--20},
pmid = {21071786},
title = {{Spatially augmented reality}},
year = {1998}
}
@article{Rolland2005,
abstract = {Distributed systems technologies supporting 3D visualization and social collaboration will be increasing in frequency and type over time. An emerging type of head-mounted display referred to as the head-mounted projection display (HMPD) was recently developed that only requires ultralight optics (i.e., less than 8 g per eye) that enables immersive multiuser, mobile augmented reality 3D visualization, as well as remote 3D collaborations. In this paper a review of the development of lightweight HMPD technology is provided, together with insight into what makes this technology timely and so unique. Two novel emerging HMPD-based technologies are then described: a teleportal HMPD (T-HMPD) enabling face-to-face communication and visualization of shared 3D virtual objects, and a mobile HMPD (M-HMPD) designed for outdoor wearable visualization and communication. Finally, the use of HMPD in medical visualization and training, as well as in infospaces, two applications developed in the ODA and MIND labs respectively, are discussed. [ABSTRACT FROM AUTHOR]},
author = {Rolland, Jannick P. and Biocca, Frank and Hamza-Lup, Felix and Ha, Yanggang and Martins, Ricardo},
doi = {10.1162/105474605774918741},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Platform/HMD/1357.pdf:pdf},
isbn = {1054-7460},
issn = {1054-7460},
journal = {Presence: Teleoperators and Virtual Environments},
number = {5},
pages = {528--549},
title = {{Development of Head-Mounted Projection Displays for Distributed, Collaborative, Augmented Reality Applications}},
volume = {14},
year = {2005}
}
@article{Wagner2006,
author = {Wagner, Daniel and Schmalstieg, Dieter},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Platform/HHD/QuirkEtAl-EDT06.pdf:pdf},
isbn = {9780798856072},
journal = {IEEE Virtual Reality Conference (VR 2006)},
title = {{Handheld augmented reality displays}},
year = {2006}
}
@article{Jebara1997,
abstract = {We propose a practical application of wearable computing and augmented reality which enhances the game of billiards. A vision algorithm is implemented which operates in interactive-time with the user to assist planning and aiming. Probabilistic color models and symmetry operations are used to localize the table, pockets and balls through a video camera near the user's eye. Classification of the objects of interest is performed and each possible shot is ranked in order to determine its relative usefulness. The system allows the user to proceed through a regular pool game while it automatically determines strategic shots. The resulting trajectories are rendered as graphical overlays on a head mounted live video display. The wearable video output and the computer vision system provide an integration of real and virtual environments which enhances the experience of playing and learning the game of billiards without encumbering the player},
author = {Jebara, T. and Eyster, C. and Weaver, J. and Starner, T. and Pentland, A.},
doi = {10.1109/ISWC.1997.629930},
file = {:Users/James/Documents/University/Advanced{\_}Computer{\_}Science/Year{\_}1/Research{\_}Techniques/Coursework/Coursework{\_}1/Research/Applications/Games/jebara-iswc97.pdf:pdf},
isbn = {0-8186-8192-6},
journal = {Digest of Papers. First International Symposium on Wearable Computers},
pages = {138--145},
title = {{Stochasticks: augmenting the billiards experience with probabilistic vision and wearable computers}},
year = {1997}
}


